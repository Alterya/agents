create a system prompt for an agents that is incharge of all the grafana alerts and services logs and statuses.
if he is asked for anything related to services logs, logs or alerts, he should be able to solve it.
questions like do i have have any errors in my service (just a log search)
to questions like check for all the alerts that sendes messages to the slack channel collectim-alerts, then for each one check the relevant services logs and explain to me why did the alerts was triggered
he should be able to give in the end a full answer to the user related to does subjects

############################ Resources ############################
* Available labels by datasource *
grafanacloud-ronalterya-logs (primary app logs):
app_kubernetes_io_name, cluster, container, filename, flags, hostname, instance, job, namespace, os, pipeline, pod, service_name, source, stream, stream_shard (internal)
grafanacloud-ronalterya-alert-state-history:
folderUID, from, group, instance_id, instance_type, orgID/org_id, service_name, stream_shard (internal)
grafanacloud-ronalterya-usage-insights:
instance_id, instance_type, org_id, stream_shard (internal)
Most recommended labels to query first
cluster: environment/cluster scoping; stable, low-cardinality.
namespace: K8s logical boundary; ideal for narrowing services.
app_kubernetes_io_name or service_name: canonical app/service identity; use one consistently.
job: scrape target (e.g., promtail job); good mid-level filter.
stream: stdout vs stderr; quick signal/noise split.
Useful for drilling down (use after coarse filters)
pod and container: precise isolation; high-cardinality—apply only after cluster/namespace/app filters.
filename: useful for node/file-path specific issues; also high-cardinality.
instance/hostname: node-scoped investigations.


* Alerts handeling *
Core concepts (in your org)
Folders: Alerts are grouped by folderUIDs like ce2ujz4az0agwd (Backend High Evaluation), ce3t28uz3uzuoe (Blockchain Monitoring High Evaluation), aetvs7uucye4ga (Jobs/Tiered), aeuyp7uzs6mm8e (Collection infra), etc. Folders help ownership and RBAC.
Rule groups: Execution buckets inside a folder. Examples:
Backend High Evaluation
Blockchain Monitoring High Evaluation
Errors
Tier 1
These define evaluation cadence and ordering.
Rule states you’ll see:
firing: Condition currently met (e.g., “backend prod alerts”, “Collection Hub BFF Errors”).
inactive: Not currently alerting; evaluates normally.
pending: Would be true but still waiting for for-duration to elapse.
no_data or error: Data source returned no data, or expression errored. Each rule has configured noDataState/execErrState that decide whether to alert.
Example live rules (how they’re built)
backend - long response times prod (firing)
Folder: Backend High Evaluation
Source: Loki
Query: count_over_time on prod alt-server logs where response_time_seconds > 8, JSON parse OK, keep relevant fields.
for: 5m; Receiver: Slack Backend Prod Alerts
States: noDataState=OK, execErrState=Error
backend prod alerts (firing)
Folder: Backend High Evaluation
Source: Loki
Query: WARNING|ERROR|EXCEPTION|CRITICAL over last window; for: 5m
Receiver: Slack Backend Prod Alerts
No Ethereum transactions in 5 minutes (firing)
Folder: Blockchain Monitoring High Evaluation
Source: PostgreSQL
Raw SQL checks last tx time; noDataState=NoData; Receiver: Slack Alerts
Collection Hub BFF Errors (firing)
Folder: Collection infra
Source: Loki; sums ERRORs for vpk-extension-backend; noDataState=NoData
Labels: service_name=collection-infra, severity=medium, sla=30m, team_name=collection-infra, tier=2
Failed jobs (firing)
Folder: Jobs (Tiered)
Source: Prometheus; kube_job_status_failed == 1
Labels: service_name=jobs, severity=low, sla=24h, team_name=devops, tier=1
Labels and routing
Many rules include labels: severity (low/medium/severe), tier (1/2/3), team_name, service_name, sla.
Contact points (routing targets) in your org include Slack channels (e.g., Slack Backend Prod Alerts, collection-infra-alerts-slack), PagerDuty, Opsgenie, Email, and OnCall.
OnCall schedules:
Platform on call (S5847TQQ6ICTR) — currently on-call: ron2588
Backend API Oncall (SSE8Z4CCSFXEE) — no shifts listed yet
Handling workflow (recommended)
Acknowledge/route:
Firing “severe/tier 3” → page Opsgenie/PagerDuty; Slack for visibility.
Medium/low or tier 1–2 → Slack channel + create issue if recurring.
Verify data health:
If state is error/no_data, check noDataState/execErrState and inspect the query’s datasource health (Loki/Prom/SQL).
Triage quickly with labels:
team_name → which team owns it
service_name → affected service
severity/tier → urgency
sla → response expectation
Standard first-look queries:
Loki: open the rule’s query and expand time window; filter by cluster/namespace/app label; check stream stderr.
Prometheus: view the raw instant query and related panels for context.
SQL: run the query manually to validate data freshness, indexes, and connectivity.
Escalation:
Use the rule’s receiver mapping (Slack/Opsgenie/PagerDuty). For platform-wide incidents, page “Platform on call”.
If noisy or false-positive: mute rule temporarily (maintenance window), and capture a follow-up task to adjust thresholds.
Hygiene and improvements
Ensure every rule has:
Owner label (team_name), severity, tier, sla, and service_name.
Receiver mapped to the right Contact Point.
for duration tuned to avoid flapping (5–10m common for logs; shorter for infra metrics).
Prefer OK on noDataState for logs-based rules that may be sparse; use KeepLast on steady infra metrics.
Group related rules into clear ruleGroups with sensible cadence to reduce evaluation thrash.
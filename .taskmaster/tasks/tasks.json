{
  "master": {
    "tasks": [],
    "metadata": {
      "created": "2025-07-29T15:57:21.028Z",
      "updated": "2025-08-11T14:29:14.350Z",
      "description": "Tasks for master context"
    }
  },
  "agent-wars": {
    "tasks": [
      {
        "id": 1,
        "title": "Shared: Database schema and Prisma models",
        "description": "Design and implement Postgres schema and Prisma models for Agent, Conversation, Message, RunReport, and PromptTemplate, including migrations, seed data for prebuilt agents, and basic data-access utilities.",
        "details": "Implementation subtasks (target 5):\n1) Implement schema + migrations\n- Use Prisma v5.x with Postgres. Entities and fields:\n  - Agent: id (cuid, PK), name (string, unique), description (string), systemPrompt (text), persona (json), createdAt, updatedAt, isActive (bool), tags (string[])\n  - Conversation: id (cuid, PK), agentId (FK->Agent), model (string), systemPrompt (text), goal (text), goalReached (bool, default false), endedReason (enum: 'goal'|'limit'|'error'|'manual'|'timeout'), messageLimit (int, default 25), messageCount (int, default 0), runId (string|null), startedAt, endedAt (date|null)\n  - Message: id (cuid, PK), conversationId (FK->Conversation), role (enum: 'system'|'user'|'assistant'|'tool'), content (text), tokensIn (int|null), tokensOut (int|null), costUsd (decimal(10,5)|null), createdAt\n  - RunReport: id (cuid, PK), runId (string, unique), agentId (FK->Agent), model (string), systemPrompt (text), runCount (int), failures (json), summary (text), revisedPrompt (text), stats (json), createdAt\n  - PromptTemplate: id (cuid, PK), name (string, unique), description (string), template (text), variables (string[]), lastUsedAt (date|null), createdAt, updatedAt\n- Indexes: Conversation(agentId), Message(conversationId, createdAt), Agent(name), RunReport(runId)\n- Prisma schema (pseudo):\n  model Agent { id String @id @default(cuid()) name String @unique description String? systemPrompt String persona Json? isActive Boolean @default(true) tags String[] @default([]) conversations Conversation[] createdAt DateTime @default(now()) updatedAt DateTime @updatedAt }\n  model Conversation { id String @id @default(cuid()) agentId String agent Agent @relation(fields: [agentId], references: [id]) model String systemPrompt String? goal String? goalReached Boolean @default(false) endedReason EndReason? messageLimit Int @default(25) messageCount Int @default(0) runId String? startedAt DateTime @default(now()) endedAt DateTime? messages Message[] @@index([agentId]) }\n  enum EndReason { goal limit error manual timeout }\n  model Message { id String @id @default(cuid()) conversationId String conversation Conversation @relation(fields: [conversationId], references: [id]) role Role content String tokensIn Int? tokensOut Int? costUsd Decimal? @db.Decimal(10,5) createdAt DateTime @default(now()) @@index([conversationId, createdAt]) }\n  enum Role { system user assistant tool }\n  model RunReport { id String @id @default(cuid()) runId String @unique agentId String agent Agent @relation(fields: [agentId], references: [id]) model String systemPrompt String? runCount Int failures Json? summary String? revisedPrompt String? stats Json? createdAt DateTime @default(now()) }\n  model PromptTemplate { id String @id @default(cuid()) name String @unique description String? template String variables String[] @default([]) lastUsedAt DateTime? createdAt DateTime @default(now()) updatedAt DateTime @updatedAt }\n- Run: npx prisma migrate dev -n init\n\n2) Seed and minimal data-access utils\n- Create prisma/seed.ts to insert 3–5 prebuilt Agents with distinct personas.\n- Add lightweight repository helpers (e.g., getActiveAgents, createConversation, appendMessage, completeConversation, saveRunReport, upsertPromptTemplate).\n\n3) Cleanup\n- Remove sample data not used in MVP, ensure nullable fields match usage, verify cascade delete behavior (prefer ON DELETE CASCADE for Message on Conversation).\n\n4) Quality Gate (run tests/lint)\n- Add scripts: prisma:generate, prisma:migrate, prisma:seed. Run typecheck. Ensure Prisma Client generated.\n\n5) Context7 MCP research for external packages/libraries used\n- Prisma v5 docs, Neon/Supabase connection pool guidance for serverless (use pgbouncer/pooler or Prisma Data Proxy). Verify Vercel Postgres/Neon best practices; ensure DATABASE_URL uses connection pooling.\n\n<info added on 2025-08-11T15:37:22.943Z>\nEnhancements (research-backed):\n\n- Schema additions and changes (Prisma v5, Postgres):\n  - Message: add seq Int (monotonically increasing per conversation), @@index([conversationId, seq]); keep existing @@index([conversationId, createdAt]). In appendMessage, assign seq transactionally as max(seq)+1 for the conversation to support incremental polling.\n  - Conversation: add tokensInTotal Int @default(0), tokensOutTotal Int @default(0), costUsdTotal Decimal? @db.Decimal(12,6) @default(0).\n  - RunReport: add tokensInTotal Int @default(0), tokensOutTotal Int @default(0), costUsdTotal Decimal? @db.Decimal(12,6) @default(0).\n  - Enums:\n    - JudgeMode { auto, human, hybrid }\n    - BattleWinner { A, B, draw, undecided }\n  - Battle:\n    - id String @id @default(cuid())\n    - conversationAId String, conversationBId String\n    - conversationA Conversation @relation(fields: [conversationAId], references: [id])\n    - conversationB Conversation @relation(fields: [conversationBId], references: [id])\n    - participantA Json?, participantB Json?\n    - goal String?\n    - winner BattleWinner?\n    - endedReason EndReason?\n    - judgeMode JudgeMode?\n    - judgeRationale String?\n    - metrics Json?\n    - isBlind Boolean @default(true)\n    - createdAt DateTime @default(now()), updatedAt DateTime @updatedAt\n    - judgeDecisions JudgeDecision[]\n    - @@index([conversationAId]), @@index([conversationBId])\n  - JudgeDecision:\n    - id String @id @default(cuid())\n    - battleId String\n    - battle Battle @relation(fields: [battleId], references: [id])\n    - evaluationRunId String?\n    - evaluationRun EvaluationRun? @relation(fields: [evaluationRunId], references: [id])\n    - judgeMode JudgeMode\n    - winner BattleWinner\n    - rationale String?\n    - metrics Json?\n    - createdAt DateTime @default(now())\n    - failureTags DecisionFailureTag[] (m:n)\n    - @@index([battleId]), @@index([evaluationRunId])\n  - EvaluationRun:\n    - id String @id @default(cuid())\n    - name String @unique\n    - description String?\n    - judgeMode JudgeMode?\n    - isBlind Boolean @default(true)\n    - config Json?\n    - stats Json?\n    - createdAt DateTime @default(now()), updatedAt DateTime @updatedAt\n    - decisions JudgeDecision[]\n    - metrics EvaluationMetric[]\n  - EvaluationMetric:\n    - id String @id @default(cuid())\n    - evaluationRunId String\n    - run EvaluationRun @relation(fields: [evaluationRunId], references: [id])\n    - key String\n    - value Float?\n    - details Json?\n    - createdAt DateTime @default(now())\n    - @@index([evaluationRunId])\n  - FailureTag:\n    - id String @id @default(cuid())\n    - slug String @unique\n    - description String?\n  - DecisionFailureTag (join table):\n    - judgeDecisionId String\n    - failureTagId String\n    - judgeDecision JudgeDecision @relation(fields: [judgeDecisionId], references: [id])\n    - failureTag FailureTag @relation(fields: [failureTagId], references: [id])\n    - @@id([judgeDecisionId, failureTagId])\n    - @@index([failureTagId])\n  - Rating:\n    - id String @id @default(cuid())\n    - subject String @unique\n    - mu Float\n    - sigma Float\n    - games Int @default(0)\n    - createdAt DateTime @default(now()), updatedAt DateTime @updatedAt\n\n- Repository helpers (add/modify):\n  - appendMessage: within a transaction, compute next seq for the conversation (SELECT max(seq)+1 FOR UPDATE or equivalent), insert Message with seq, and increment Conversation.messageCount plus aggregate totals (tokensInTotal, tokensOutTotal, costUsdTotal).\n  - getMessagesSinceSeq(conversationId: string, sinceSeq: number, limit = 100): returns messages where seq > sinceSeq ordered by seq asc (uses @@index([conversationId, seq])).\n  - completeConversation: no change beyond existing behavior.\n  - saveRunReport: accept tokensInTotal?, tokensOutTotal?, costUsdTotal? and persist; if not provided, compute aggregates from related conversations/messages where applicable.\n  - Battle helpers: createBattle({ conversationAId, conversationBId, participantA?, participantB?, goal?, isBlind? }), recordJudgeDecision({ battleId, evaluationRunId?, judgeMode, winner, rationale?, metrics?, failureTagSlugs?: string[] }).\n  - Rating helpers: upsertRating({ subject, mu, sigma, games? }).\n\n- Indexing:\n  - Maintain Message @@index([conversationId, createdAt]).\n  - Add Message @@index([conversationId, seq]). If seq cannot be set for legacy messages, fallback read path may use @@index([conversationId, id]) temporarily.\n\n- Cost and usage aggregation:\n  - Store per-message tokens and cost as already defined; update Conversation and RunReport aggregate totals on each message append and on run report save. Ensure Decimal precision is sufficient for totals (12,6 recommended).\n\n- Serverless Prisma pooling and env:\n  - Use pooled DATABASE_URL with pgbouncer=true, connection_limit=1, sslmode=require, connect_timeout=5 (or similar). Use DIRECT_URL (non-pooled) for prisma migrate/studio.\n  - Consider Prisma Accelerate/Data Proxy for Edge or higher concurrency; if enabled, add the accelerate generator and route reads accordingly.\n  - Example:\n    - DATABASE_URL=postgresql://user:pass@pool-host/db?sslmode=require&pgbouncer=true&connection_limit=1&connect_timeout=5\n    - DIRECT_URL=postgresql://user:pass@direct-host/db?sslmode=require\n\n- Migrations:\n  - Create a follow-up migration adding new models, enums, fields, and indexes (e.g., npx prisma migrate dev -n eval_battles_seq_aggregates). Backfill seq for existing messages per conversation using a window function or application-side script.\n\n- Tests (augment):\n  - Verify Message.seq is strictly increasing per conversation under concurrency; getMessagesSinceSeq returns correct window.\n  - Validate aggregate totals update on Conversation and RunReport.\n  - CRUD for Battle, JudgeDecision, EvaluationRun, EvaluationMetric, FailureTag, Rating; m:n tagging works.\n  - Index usage: EXPLAIN on messages since seq and standard read path.\n\n- Research references:\n  - Add .taskmaster/docs/research notes summarizing Prisma pooling (pgbouncer, pooled vs direct URLs, connection_limit) and the evaluation/battle schema rationale with links to source docs and examples.\n</info added on 2025-08-11T15:37:22.943Z>\n<info added on 2025-08-12T08:46:07.814Z>\nProject-wide tooling baseline (free/local) to add:\n\n- Linters (ESLint)\n  - Dev deps: eslint, eslint-config-next, @typescript-eslint/eslint-plugin, @typescript-eslint/parser, eslint-plugin-react, eslint-plugin-react-hooks, eslint-plugin-jsx-a11y, eslint-plugin-import-x (or eslint-plugin-import), eslint-config-prettier, eslint-plugin-tailwindcss\n  - .eslintrc: extends next/core-web-vitals, plugin:@typescript-eslint/recommended, plugin:react/recommended, plugin:react-hooks/recommended, plugin:jsx-a11y/recommended, plugin:import-x/recommended, plugin:tailwindcss/recommended, prettier\n  - Rules: tailwindcss/no-contradicting-classname error; import/order via import-x with groups and alphabetize; react/react-in-jsx-scope off; no restricted imports to enforce relative path hygiene as needed\n  - Parser: @typescript-eslint/parser with project tsconfig; env browser, node, es2022, jest; settings react version detect; ignore patterns for .next, dist, coverage, prisma/generated\n\n- Formatters (Prettier)\n  - Dev deps: prettier, prettier-plugin-tailwindcss\n  - .prettierrc: include prettier-plugin-tailwindcss and standard style (semi true, singleQuote true, trailingComma all)\n  - Ensure eslint-config-prettier is last in ESLint extends\n\n- Unit/integration tests\n  - Dev deps: vitest, @testing-library/react, @testing-library/user-event, @testing-library/jest-dom, msw, @vitest/coverage-v8\n  - vitest config: environment jsdom; setupFiles to load jest-dom and MSW server (start/stop/reset handlers)\n  - Add tests covering repository helpers against a test Postgres (Docker or Neon branch), aligned with this task’s test strategy\n\n- E2E and QA\n  - Dev deps: @playwright/test, @axe-core/playwright, @lhci/cli\n  - Playwright config with projects for Chromium/WebKit/Firefox; global a11y check via axe on key pages\n  - Lighthouse CI: add lighthouserc with PRD-defined budgets (e.g., TTI, TBT, CLS, LCP thresholds)\n\n- Prisma workflow\n  - Standardize scripts for prisma format, validate, migrate, generate, seed\n  - Test DB options: Docker Postgres service (preferred locally) or Neon branch URL; use DIRECT_URL for migrate/studio and pooled DATABASE_URL for runtime\n\n- Optional Python worker (if enabled)\n  - Tools: ruff, black, isort, pytest\n  - Config files: pyproject.toml with tool.black, tool.isort (profile black), tool.ruff rules; pytest.ini basic config\n  - Make targets to lint/format/test Python code under ./worker or similar\n\n- Makefile (top-level) to orchestrate common flows\n  - install: package manager install\n  - lint: ESLint; lint:fix: ESLint with fix\n  - format and format:check: Prettier\n  - typecheck: tsc --noEmit\n  - test: Vitest run; test:watch; coverage\n  - e2e: Playwright run; e2e:report\n  - prisma:generate, prisma:validate, prisma:migrate, prisma:seed\n  - db:up, db:down, db:reset for local Docker Postgres\n  - a11y: Playwright + axe run on critical routes\n  - lighthouse: lhci autorun using budgets\n  - ci: sequence covering generate -> migrate (DIRECT_URL) -> seed -> typecheck -> lint -> unit tests -> e2e (optional) -> lhci\n\n- package.json scripts (baseline)\n  - lint, lint:fix, format, format:check, typecheck, test, test:watch, coverage, e2e, e2e:report, prisma:generate, prisma:validate, prisma:migrate, prisma:seed, lhci\n  - prisma seed hook: { \"prisma\": { \"seed\": \"tsx prisma/seed.ts\" } }\n\n- Output Policy for Prompt agents\n  - No hidden reasoning in outputs\n  - If a required fact or tool is missing and the user insists, reply exactly: information unavailable\n\nRefer to PRD section “Local Tooling Stack” for thresholds, conventions, and any additional rules to mirror across repos.\n</info added on 2025-08-12T08:46:07.814Z>\n<info added on 2025-08-12T09:09:36.786Z>\nQuality-Gate Loop (pre-commit enforced)\n- a) Cleanup\n  - Remove redundant/unused files, dead code paths, and obsolete sample data.\n  - Update .gitignore to exclude: node_modules, .next, dist, coverage, prisma/generated, .env, .env.*, .DS_Store, tmp, .vercel, .turbo (if present).\n  - Update README: quickstart for Prisma (migrate/generate/seed), pooled vs DIRECT_URL env usage, test DB setup, and the Quality-Gate workflow below.\n- b) Self-Review\n  - Inspect the full diff. Verify each subtask deliverable exists, compiles, and is coherent:\n    - prisma/schema.prisma reflects all models/enums/indexes including seq, aggregates, and evaluation/battle entities.\n    - Migrations: initial + follow-up (eval_battles_seq_aggregates) present; backfill plan/script for seq noted.\n    - Seed inserts 3–5 agents with distinct personas.\n    - Repository helpers implemented with transactions and basic error handling; tests cover core paths and concurrency for seq.\n    - Scripts in package.json and Makefile targets exist and run (including prisma:generate/migrate/seed).\n    - .env.example includes DATABASE_URL (pooled) and DIRECT_URL (non-pooled).\n  - Run locally: make quality and make test; both must pass.\n- c) Git add & commit (DO NOT PUSH)\n  - Stage logically grouped changes and create descriptive commits (e.g., prisma: initial schema+migration; repo: helpers+tests; migrations: eval/battles/aggregates; seed: prebuilt agents; tooling: Makefile+pre-commit; docs: README+.env.example).\n  - Use conventional “scope: summary” style with concise bodies; note any breaking changes.\n- Pre-commit enforcement\n  - Add a pre-commit hook that runs: make quality && make test.\n  - If using Husky: dev-install husky and add .husky/pre-commit with the above command. If not, add .git/hooks/pre-commit shell script invoking the same command (ensure executable).\n  - Makefile: add a quality target (or alias quality to check) that runs prisma format, prisma validate, typecheck, eslint (no fix), and prettier --check to ensure consistency before commit.\n</info added on 2025-08-12T09:09:36.786Z>",
        "testStrategy": "- Unit: Vitest tests for repository helpers using a test Postgres (Docker) or Neon branch DB.\n- Migration validation: Apply migrations on a fresh DB, run basic CRUD flows.\n- Seed sanity: Ensure at least one Agent present and selectable.\n- Performance: Verify indexes via EXPLAIN on Message read path.\n- Rollback: Test down migration and re-apply without data loss for non-critical fields.",
        "priority": "high",
        "dependencies": [],
        "status": "review",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Prisma schema and initial migration",
            "description": "Define Prisma v5 schema for Agent, Conversation, Message, RunReport, and PromptTemplate with enums, relations, indexes, defaults, and cascade rules. Generate the initial migration and Prisma Client.",
            "dependencies": [],
            "details": "Tasks:\n- Create prisma/schema.prisma with datasource (provider: postgresql) and generator client.\n- Models:\n  - Agent: id(cuid, PK), name(unique), description?, systemPrompt, persona(Json?), isActive(default true), tags(String[] default []), conversations(Conversation[]), createdAt(now), updatedAt(@updatedAt).\n  - Conversation: id(cuid), agentId(FK->Agent), model, systemPrompt?, goal?, goalReached(default false), endedReason(enum EndReason?), messageLimit(default 25), messageCount(default 0), runId?, startedAt(now), endedAt?, messages(Message[]); @@index([agentId]).\n  - Message: id(cuid), conversationId(FK->Conversation onDelete: Cascade), role(enum Role), content(text), tokensIn?, tokensOut?, costUsd Decimal @db.Decimal(10,5)?, createdAt(now); @@index([conversationId, createdAt]).\n  - RunReport: id(cuid), runId(unique), agentId(FK->Agent), model, systemPrompt?, runCount, failures(Json?), summary?, revisedPrompt?, stats(Json?), createdAt(now).\n  - PromptTemplate: id(cuid), name(unique), description?, template, variables(String[] default []), lastUsedAt?, createdAt(now), updatedAt(@updatedAt).\n  - Enums: EndReason { goal, limit, error, manual, timeout }, Role { system, user, assistant, tool }.\n- Create indexes per spec: Conversation(agentId), Message(conversationId, createdAt), Agent(name unique), RunReport(runId unique).\n- Run: npx prisma migrate dev -n init; then npx prisma generate.\n\nTag: backend\n\n<plan added on 2025-08-13T08:15:07.765Z>\nPlan for 1.1 (Implement Prisma schema and initial migration):\n- Create prisma/schema.prisma with models: Agent, Conversation, Message, RunReport, PromptTemplate; enums Role, EndReason.\n- Add @prisma/client to dependencies and tsx to devDependencies in agents_wars/web/package.json.\n- Add prisma seed hook: { \"prisma\": { \"seed\": \"tsx prisma/seed.ts\" } }.\n- Create prisma/seed.ts to insert 3–5 prebuilt Agents.\n- Run: npm run prisma:format && npm run prisma:generate.\n- Prepare .env with DATABASE_URL (pooled) and DIRECT_URL for future migrations.\n\n<log added on 2025-08-13T08:29:05.834Z>\nNon-Docker setup for Prisma (macOS/local):\n1) Install a local Postgres (e.g., Postgres.app or Homebrew).\n2) Create DB/user (example):\n   createdb agentwars\n   psql -d postgres -c \"CREATE USER agentwars WITH PASSWORD 'agentwars';\"\n   psql -d postgres -c \"GRANT ALL PRIVILEGES ON DATABASE agentwars TO agentwars;\"\n3) Update src/agents_wars/web/.env with your real credentials for DATABASE_URL and DIRECT_URL.\n4) From web/:\n   - npm run prisma:migrate  (first run will create initial migration)\n   - npm run prisma:seed     (optional)\n   - npm run prisma:studio   (optional)\n\nWhen DB is configured, I will run migrate + seed and complete 1.1.\n",
            "status": "review",
            "testStrategy": "Validate schema: npx prisma validate. Apply migration on a fresh DB (Docker Postgres or Neon branch). Verify relations and indexes exist. Delete a Conversation and confirm its Messages are cascade-deleted. Quick CRUD smoke via Prisma Client."
          },
          {
            "id": 2,
            "title": "Seed prebuilt agents and implement minimal repository helpers",
            "description": "Add seed script to insert 3–5 prebuilt Agents with distinct personas, and implement lightweight data-access utilities for common operations.",
            "dependencies": [
              "1.1"
            ],
            "details": "Tasks:\n- Create prisma/seed.ts inserting 3–5 Agents (e.g., Generalist, Prompt Tuner, Evaluator, Researcher, Code Reviewer) with unique name, systemPrompt, persona JSON, tags.\n- Implement repository helpers (e.g., src/lib/db or src/repo):\n  - getActiveAgents(): Promise<Agent[]>.\n  - createConversation({ agentId, model, systemPrompt?, goal?, messageLimit? }): Promise<Conversation>.\n  - appendMessage(conversationId, { role, content, tokensIn?, tokensOut?, costUsd? }): Promise<Message> (increment messageCount in a transaction).\n  - completeConversation(conversationId, { endedReason, goalReached, endedAt? }): Promise<Conversation>.\n  - saveRunReport({ runId, agentId, model, systemPrompt?, runCount, failures?, summary?, revisedPrompt?, stats? }): Promise<RunReport>.\n  - upsertPromptTemplate({ name, description?, template, variables }): Promise<PromptTemplate> (upsert by unique name).\n- Ensure types are inferred from Prisma Client; add basic error handling for unique constraint violations.\n\nTag: backend",
            "status": "review",
            "testStrategy": "Manual: Run seed and verify Agents exist. Programmatic: Create a conversation, append messages, check messageCount increments, complete conversation updates status, upsert prompt template returns same id on repeat."
          },
          {
            "id": 3,
            "title": "Cleanup and alignment with MVP",
            "description": "Remove unused sample data, confirm nullable fields and defaults match usage, verify cascade behavior and indexes, and adjust schema if needed.",
            "dependencies": [
              "1.1",
              "1.2"
            ],
            "details": "Tasks:\n- Remove any unused sample models/data. Confirm optional fields (e.g., description?, systemPrompt? on Conversation, RunReport) and defaults (goalReached=false, messageLimit=25, messageCount=0).\n- Verify onDelete: Cascade for Message->Conversation; keep Agent->Conversation as Restrict (or explicit NoAction) to avoid accidental loss; document behavior.\n- Re-run prisma format and generate a follow-up migration if tweaks are required.\n- Validate indexes: Conversation(agentId), Message(conversationId, createdAt), Agent(name unique), RunReport(runId unique).\n\nTag: backend",
            "status": "review",
            "testStrategy": "Apply updated migration on a new DB; run EXPLAIN on common queries (messages by conversation ordered by createdAt desc limit 20) to confirm index usage. Delete a Conversation and ensure Messages cascade; attempt deleting an Agent with Conversations and ensure it is blocked (if Restrict)."
          },
          {
            "id": 4,
            "title": "Quality gate: scripts, generate client, typecheck, lint, and repository tests",
            "description": "Add package scripts, ensure Prisma Client generation, and run typecheck/lint plus Vitest repository tests against a test Postgres.",
            "dependencies": [
              "1.1",
              "1.2",
              "1.3"
            ],
            "details": "Tasks:\n- package.json scripts: prisma:generate (prisma generate), prisma:migrate (prisma migrate dev), prisma:seed (prisma db seed), test (vitest run), typecheck (tsc --noEmit), lint (eslint .).\n- Configure Prisma seed hook: package.json { \"prisma\": { \"seed\": \"tsx prisma/seed.ts\" } } (or ts-node).\n- Test DB: Docker Postgres or Neon branch; set DATABASE_URL (dev), DATABASE_URL_TEST (CI). Use DIRECT_URL for migrations if using a pooler.\n- Vitest: tests for getActiveAgents, createConversation + appendMessage (transaction + counter), completeConversation (ended flags/reason), upsertPromptTemplate (id stable on repeat), saveRunReport (unique runId).\n- Ensure prisma generate runs before tests and CI uses fresh DB with migrations and seed.\n\nTag: backend",
            "status": "review",
            "testStrategy": "Run sequence: prisma:generate -> prisma:migrate (against test DB) -> prisma:seed -> typecheck -> lint -> vitest. Assertions: all repos function; no unique/index violations; Prisma Client imports compile."
          },
          {
            "id": 5,
            "title": "Context7 MCP research: Prisma v5 + serverless Postgres pooling",
            "description": "Research and document best practices for Prisma v5 with Postgres on Neon/Supabase/Vercel, focusing on connection pooling and environment configuration.",
            "dependencies": [],
            "details": "Scope:\n- Docs to review: Prisma v5 (PostgreSQL, Accelerate/Data Proxy), Neon pgbouncer/pooler, Vercel Postgres with Prisma, Supabase pooled connections.\n- Recommendations:\n  - Use a pooled DATABASE_URL in serverless (Neon/Vercel pooler or Supabase pooler port 6543) to prevent connection storms; use DIRECT_URL for migrations.\n  - For PgBouncer, add connection string params: pgbouncer=true, sslmode=require, connection_timeout/pgbouncer settings; disable prepared statements if needed via ?pgbouncer=true.\n  - Consider Prisma Accelerate/Data Proxy in high-concurrency serverless; configure generator accelerate if adopted.\n  - CI/CD: run migrations in a controlled step using DIRECT_URL (non-pooled) to avoid pooler limitations.\n- Example env:\n  - DATABASE_URL=postgresql://user:pass@neon-pool-host/db?sslmode=require&pgbouncer=true&connect_timeout=5\n  - DIRECT_URL=postgresql://user:pass@neon-direct-host/db?sslmode=require\n  - For Supabase: use pooled url (port 6543) as DATABASE_URL; non-pooled as DIRECT_URL for migrations.\n- Notes: keep relationMode=foreignKeys; ensure Decimal(10,5) is supported; avoid long-running transactions in serverless.\n\nTag: backend\n\n<research added on 2025-08-13T09:14:02.798Z>\nPrisma v5 + serverless Postgres pooling notes saved to .taskmaster/docs/research/prisma-pooling-notes.md.\n- Use pooled DATABASE_URL at runtime; DIRECT_URL for migrations/studio.\n- Add pgbouncer and ssl params; avoid long transactions; set low connection_limit in serverless.\n- Consider Accelerate/Data Proxy for Edge.\n",
            "status": "review",
            "testStrategy": "Manual validation: run a 50-parallel request test using repository helpers against pooled DATABASE_URL and confirm no 'too many connections' errors. Run migrations using DIRECT_URL to ensure compatibility with pooler. Document findings and finalize env templates."
          },
          {
            "id": 6,
            "title": "Makefile: lint, format, typecheck, unit, e2e, prisma, check, ci",
            "description": "Create a Makefile to orchestrate local and CI commands for lint/format/typecheck/tests/e2e and Prisma utilities.",
            "details": "Targets (pnpm examples):\\n- install: pnpm install\\n- dev: pnpm dev\\n- build: pnpm build\\n- typecheck: pnpm tsc --noEmit\\n- lint: pnpm eslint .\\n- lint-fix: pnpm eslint . --fix\\n- format: pnpm prettier --write \"**/*.{js,jsx,ts,tsx,md,css,scss,json}\"\\n- format-check: pnpm prettier --check \"**/*.{js,jsx,ts,tsx,md,css,scss,json}\"\\n- test: pnpm vitest run\\n- test-watch: pnpm vitest\\n- e2e: pnpm playwright test\\n- e2e-headed: pnpm playwright test --headed\\n- prisma-format: pnpm prisma format\\n- prisma-validate: pnpm prisma validate\\n- prisma-migrate: pnpm prisma migrate dev -n $(name)\\n- prisma-seed: pnpm prisma db seed\\n- prisma-studio: pnpm prisma studio\\n- check: make prisma-format prisma-validate typecheck lint test\\n- ci: pnpm prisma format && pnpm prisma validate && pnpm typecheck && pnpm eslint . && pnpm vitest run && pnpm playwright test --reporter=line\\nNotes: Only OpenAI and OpenRouter providers are supported. Optional Python worker targets (ruff/black/pytest) may be added later under a feature flag.\n\nTag: backend",
            "status": "review",
            "dependencies": [],
            "parentTaskId": 1
          }
        ]
      },
      {
        "id": 2,
        "title": "Shared: LLM provider abstraction (OpenAI + OpenRouter) with unified chat and cost/safety guards",
        "description": "Create a provider layer that unifies chat completion across OpenAI and OpenRouter, exposing a single interface with model selection, token caps, and basic rate limiting. Include retries and optional streaming support.",
        "details": "Implementation subtasks (target 5):\n1) Implement provider interface and adapters\n- Install openai@^4.55.0 (latest major v4). Use the same client for OpenRouter by overriding baseURL and apiKey.\n- Interface:\n  type ChatMessage = { role: 'system'|'user'|'assistant'|'tool', content: string };\n  type ChatOptions = { provider: 'openai'|'openrouter', model: string, maxTokens?: number, temperature?: number, stop?: string[], stream?: boolean };\n  async function chat(messages: ChatMessage[], opts: ChatOptions): Promise<{ text: string, usage?: { inputTokens: number, outputTokens: number }, raw: any }>\n- OpenAI example:\n  const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });\n  const resp = await client.chat.completions.create({ model: opts.model, messages, max_tokens: opts.maxTokens ?? 512, temperature: opts.temperature ?? 0.2 });\n- OpenRouter example:\n  const client = new OpenAI({ apiKey: process.env.OPENROUTER_API_KEY, baseURL: 'https://openrouter.ai/api/v1' });\n  const resp = await client.chat.completions.create({ model: opts.model, messages, max_tokens: opts.maxTokens ?? 512, temperature: opts.temperature ?? 0.2, extra_headers: { 'HTTP-Referer': process.env.OPENROUTER_SITE || '', 'X-Title': 'Agent Wars' } });\n- Map usage tokens from resp.choices[0] and resp.usage.\n\n2) Cost/safety guards and retries\n- Enforce caps: maxTokens per call <= 512 (configurable), max messages per conversation <= 25, allowed models whitelist.\n- Basic rate limiting with in-memory LRU (per IP/per API key) using lru-cache; fallback no-op in serverless multi-instance, but keep caps. Exponential backoff retries with abort after 2 retries on 429/5xx.\n- Redact secrets from prompts via simple regex before logging.\n\n3) Cleanup\n- Centralize env validation with zod: OPENAI_API_KEY? OPENROUTER_API_KEY? Ensure helpful error messages.\n\n4) Quality Gate (run tests/lint)\n- Vitest unit tests mocking OpenAI client. Cases: OpenAI vs OpenRouter path, token cap enforced, retry on 429, model not allowed.\n\n5) Context7 MCP research for external packages/libraries used\n- OpenAI Node SDK v4 docs (Responses vs Chat Completions—use chat.completions for broader model support). OpenRouter API compatibility notes. lru-cache best practices in serverless environments.\n\n<info added on 2025-08-11T15:38:40.961Z>\nResearch-backed enhancements to provider layer and guards:\n\n- OpenRouter integration\n  - Include required headers HTTP-Referer and X-Title on every OpenRouter request (chat and models).\n  - Add GET /models fetch with 6h TTL cache (lru-cache). Cache fields: model id, context_length (tokens), and pricing.prompt/completion (USD per 1M tokens). Expose getModelInfo(model) -> { contextTokens, promptUSDPerMTok, completionUSDPerMTok }.\n  - Normalize all pricing computations to per-1M tokens: cost = (prompt_tokens/1e6)*promptUSDPerMTok + (completion_tokens/1e6)*completionUSDPerMTok. Surface context length via getModelInfo for guard checks and observability.\n  - For OpenAI models, seed a static pricing map for common models with env override (OPENAI_PRICING_JSON) and include context lengths; fall back to conservative defaults if unknown.\n\n- Guardrails and limits\n  - Preflight token estimate using tokenizer encodings:\n    - o200k_base for gpt-4o family (gpt-4o, gpt-4.1, o3, mini variants).\n    - cl100k_base for gpt-4 (legacy), gpt-3.5, and similar.\n    - Fallback heuristic for unknown models. Reject when estimated inputTokens + requested maxTokens > model contextTokens.\n  - Budget caps: enforce MAX_USD_PER_REQUEST and MAX_USD_PER_SESSION (config). Estimate pre-call cost using pricing and token estimate; hard-stop if exceeding budget. Session budget sourced from persisted Conversation totals when available; allow per-call override (opts.budgetUSD) for ephemeral sessions.\n  - Model whitelist remains required; also validate against the cached /models catalog when provider=openrouter to catch unavailable models.\n  - Concurrency limiter keyed by provider:model with configurable limits:\n    - Defaults via CONCURRENCY_DEFAULT (e.g., 4).\n    - Per-key overrides via CONCURRENCY_OVERRIDES CSV (e.g., openai:gpt-4o=2,openrouter:anthropic/claude-3-haiku=10).\n    - Implement a keyed semaphore/queue to bound concurrent calls fairly.\n  - Retries/backoff: up to 2 retries on 429/5xx with exponential backoff and full jitter. If Retry-After header is present, honor it (seconds or HTTP-date), capped by a MAX_BACKOFF_MS ceiling. Include jitter on all backoff delays.\n\n- Streaming API\n  - Add chatStream(messages, opts) for streaming use-cases:\n    - opts.streamMode: 'iterator' returns an AsyncIterable of delta chunks.\n    - opts.streamMode: 'web' returns a Web ReadableStream suitable for Next.js/SSE routes.\n    - Preserve existing chat() for non-streaming; both paths share guards and retries (retry only on initial call).\n  - Enforce per-request timeout with AbortController (REQUEST_TIMEOUT_MS, default 60000) applied to streaming and non-streaming. Ensure streams terminate within ~60s; abort and surface a timeout error if exceeded.\n\n- Usage and cost accounting\n  - Reconcile token usage via response.usage (prompt_tokens, completion_tokens). For streaming, capture final usage when provided by SDK; if missing, fall back to preflight estimate for budgeting only and mark usageEstimated=true.\n  - Compute usdIn/usdOut/totalUSD using normalized per-1M pricing from getModelInfo or OpenAI pricing map.\n  - Persist on Message: inputTokens, outputTokens, usdIn, usdOut, totalUSD, provider, model, usageEstimated flag. Aggregate on Conversation and RunReport (incremental totals and counts).\n\n- Tests (extend Vitest suite)\n  - OpenRouter: assert HTTP-Referer and X-Title headers on chat and models requests; verify /models cache with ~6h TTL (no refetch within TTL, refetch after expiry).\n  - Guardrails: token estimation blocks requests exceeding context; budget caps reject when estimated cost exceeds per-request/session budget; whitelist rejections; concurrency limiter never exceeds configured parallelism.\n  - Retries/backoff: validate exponential backoff with jitter and honoring Retry-After (use fake timers).\n  - Pricing: verify normalization per 1M tokens and correct USD computation from a sample usage.\n  - Streaming: chatStream returns AsyncIterable and Web ReadableStream variants; iterator yields ordered deltas; timeout aborts long-running streams.\n\n- Config additions\n  - REQUEST_TIMEOUT_MS (default 60000), PRICING_CACHE_TTL_HOURS (default 6), MAX_USD_PER_REQUEST, MAX_USD_PER_SESSION, CONCURRENCY_DEFAULT, CONCURRENCY_OVERRIDES, OPENAI_PRICING_JSON (optional), RETRY_MAX_BACKOFF_MS (cap for Retry-After and exponential backoff).\n\n- References for research doc (subtask 5)\n  - OpenRouter best practices: required headers, /models endpoint schema, pricing fields, context_length.\n  - OpenAI pricing and token estimation: official pricing tables and tokenizer encodings (cl100k_base, o200k_base) and guidance on usage.\n</info added on 2025-08-11T15:38:40.961Z>\n<info added on 2025-08-12T08:47:02.728Z>\nTooling baseline and testing alignment\n- Adopt repo-wide tooling: ESLint (configs: @typescript-eslint/recommended+stylistic, eslint-plugin-react/recommended, react-hooks/recommended, jsx-a11y/recommended, import/recommended+typescript, eslint-config-prettier), Prettier with prettier-plugin-tailwindcss, Vitest, React Testing Library, MSW, Playwright with axe-core integration, and Lighthouse CI.\n- Provider-layer tests must use Vitest with full SDK mocks (mock the openai module and baseURL/header branching; no real network). MSW may be used for ancillary HTTP (e.g., OpenRouter /models) but still mocked/deterministic in unit tests.\n- Add an e2e smoke via Playwright that runs behind mocked routes only (use Playwright route interception or MSW in browser) to verify provider API endpoints and streaming behavior without hitting external LLMs.\n- Integrate Lighthouse CI in CI pipeline (not specific to provider UI but enforced repo-wide thresholds); add axe checks to Playwright flows.\n\nPrompt Agent Output Policy compliance\n- No hidden reasoning: never log, persist, or emit chain-of-thought/internal rationale. Redaction already applies to prompts; extend it to strip any reasoning-style traces from logs/analytics if present.\n- Provide a reusable policy system message constant for callers (POLICY_SYSTEM_PROMPT) that instructs models to not reveal chain-of-thought and to summarize reasoning only if needed.\n- When a required fact or tool is unavailable and the caller explicitly requests hidden reasoning or insists on it, the provider-facing helper should return exactly: information unavailable. Expose a lightweight enforcePolicy(content: string, context: { missingToolOrFact: boolean; userInsists: boolean }) utility used by higher layers before sending or after receiving model text to comply with the PRD “Local Tooling Stack”.\n</info added on 2025-08-12T08:47:02.728Z>\n<info added on 2025-08-12T09:10:07.364Z>\nQuality-Gate Loop\n- a) Cleanup: remove redundant/scaffold files and dead code; prune unused mocks/fixtures; update .gitignore to exclude build/test artifacts (e.g., .env.*, .next, dist, coverage, playwright-report, .lighthouseci) and add any new ones introduced; refresh README to document new env/config keys (REQUEST_TIMEOUT_MS, PRICING_CACHE_TTL_HOURS, MAX_USD_PER_REQUEST, MAX_USD_PER_SESSION, CONCURRENCY_DEFAULT, CONCURRENCY_OVERRIDES, OPENAI_PRICING_JSON, RETRY_MAX_BACKOFF_MS), setup, and test/run instructions.\n- b) Self-Review: inspect the diff and verify each subtask’s code exists and is sane:\n  - Provider adapters: OpenAI/OpenRouter routing, required OpenRouter headers, chat() and chatStream() behavior, usage mapping.\n  - Guards: caps, whitelist, model catalog validation, token preflight/context checks, budget caps, concurrency limiter, retries/backoff honoring Retry-After, redaction, POLICY_SYSTEM_PROMPT and enforcePolicy utility, streaming timeout.\n  - Config: zod validation with helpful errors and defaults; single source of truth used by provider/guards.\n  - Tests: Vitest with openai SDK mocks; MSW for /models; fake timers for backoff; pricing normalization; streaming iterator/web modes; rate limiting and concurrency bounds.\n  - Docs/tooling: research doc present; repo tooling aligned with baseline.\n- c) Git add & commit: stage logical chunks and create descriptive commits summarizing work per area (e.g., provider, guards, config, tests, docs, tooling). DO NOT PUSH.\n\nPre-commit enforcement\n- Add a repo pre-commit hook that runs: make quality and make test; block commits on failure. Document hook installation in README (e.g., .git/hooks/pre-commit script or Husky).\n</info added on 2025-08-12T09:10:07.364Z>",
        "testStrategy": "- Unit tests validate both providers and edge cases (caps, errors, retries).\n- Contract tests: Snapshot minimal request/response mapping.\n- Smoke test against real APIs gated by CI secret flags (avoid cost by running only on demand).\n- Security: Ensure no PII/logging of prompt content beyond hashes in tests.",
        "priority": "high",
        "dependencies": [],
        "status": "review",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement provider interface and adapters (OpenAI + OpenRouter)",
            "description": "Create a unified chat() using OpenAI Node SDK v4 for both OpenAI and OpenRouter, supporting non-streaming and optional streaming, with normalized response mapping.",
            "dependencies": [],
            "details": "Install openai@^4.55.0. Define types: type ChatMessage = { role: 'system'|'user'|'assistant'|'tool', content: string }; type ChatOptions = { provider: 'openai'|'openrouter', model: string, maxTokens?: number, temperature?: number, stop?: string[], stream?: boolean }. Export async function chat(messages: ChatMessage[], opts: ChatOptions): Promise<{ text: string, usage?: { inputTokens: number, outputTokens: number }, raw: any }>. Use the same OpenAI client for both providers; for OpenAI: new OpenAI({ apiKey: process.env.OPENAI_API_KEY }); for OpenRouter: new OpenAI({ apiKey: process.env.OPENROUTER_API_KEY, baseURL: 'https://openrouter.ai/api/v1' }). For OpenRouter, pass extra_headers: { 'HTTP-Referer': process.env.OPENROUTER_SITE || '', 'X-Title': 'Agent Wars' }. Call client.chat.completions.create({ model: opts.model, messages, max_tokens: opts.maxTokens ?? 512, temperature: opts.temperature ?? 0.2, stop: opts.stop, stream: opts.stream }). Non-streaming: return choices[0].message.content || ''. Streaming: consume streamed chunks, append delta content to build final text, and capture final usage when available; resolve with assembled text and usage. Normalize usage to { inputTokens: resp.usage?.prompt_tokens ?? 0, outputTokens: resp.usage?.completion_tokens ?? 0 }. Return { text, usage, raw: resp }.\n\nTag: backend\n\n<plan added on 2025-08-13T09:16:53.883Z>\nPlan for 2.1:\n- Add openai@^4.55.0 to web.\n- Implement src/lib/llm/provider.ts with unified chat() supporting openai and openrouter via baseURL switch.\n- Export types ChatMessage, ChatOptions, and chat().\n- Typecheck and simple smoke test later.\n",
            "status": "review",
            "testStrategy": "Manual smoke script (dev-only) to hit both providers with a short prompt; verify text returned and usage fields present when non-streaming. Full unit tests added in subtask 4."
          },
          {
            "id": 2,
            "title": "Add cost/safety guards, rate limiting, and retries",
            "description": "Enforce token and message caps, model whitelist, basic in-memory rate limiting, exponential backoff retries on 429/5xx, and prompt redaction before logging.",
            "dependencies": [
              "2.1"
            ],
            "details": "Implement guard layer wrapping chat(): caps: maxTokens per call <= 512 (configurable via env, default 512); max messages per conversation <= 25; reject if exceeded. Enforce allowed models whitelist from env (e.g., ALLOWED_MODELS=comma-separated); default to a safe small set if unset. Add simple in-memory rate limiter using lru-cache keyed by IP or API key with sliding window (e.g., 60 requests/60s configurable) and TTL; if serverless multi-instance detected, allow opt-out via RATE_LIMIT_ENABLED=false but keep caps. Implement exponential backoff with jitter for 429/5xx: up to 2 retries (total 3 attempts) with delays ~250ms, 750ms. Redact secrets in any logs via regex (e.g., api keys, bearer tokens, emails) before printing. Expose minimal metadata (attempts, provider) in raw for observability. Ensure streaming path also passes through guards and retries (retry only on initial call errors, not mid-stream).\n\nTag: backend",
            "status": "review",
            "testStrategy": "Unit: simulate inputs exceeding caps; assert rejection. Mock client to throw 429 then succeed; assert 2 retries max and delay sequencing (use fake timers). Validate whitelist rejection for disallowed model. Validate rate limiter blocks after threshold."
          },
          {
            "id": 3,
            "title": "Cleanup: Centralize environment and config validation",
            "description": "Create a config module using zod to validate environment variables and runtime options with clear error messages.",
            "dependencies": [
              "2.1"
            ],
            "details": "Add config.ts with zod schemas: At least one of OPENAI_API_KEY or OPENROUTER_API_KEY must be present; OPENROUTER_SITE optional; ALLOWED_MODELS optional CSV -> string[]; MAX_TOKENS_PER_CALL default 512; MAX_MESSAGES_PER_CONVO default 25; RATE_LIMIT_ENABLED default true; RATE_LIMIT_RPM default 60. Export getConfig() returning typed config and helpers. Validate at module load and throw descriptive errors with remediation hints. Replace scattered env reads in provider/guards with config getters.\n\nTag: backend",
            "status": "done",
            "testStrategy": "Unit: when neither API key is set, expect a precise zod error; when only one is set, config loads; CSV parsing produces array; numeric envs parsed and clamped to sane bounds."
          },
          {
            "id": 4,
            "title": "Quality Gate: tests and lint",
            "description": "Add Vitest unit tests mocking OpenAI client, plus lint/format checks. Cover provider routing, caps, retries, whitelist, and streaming assembly.",
            "dependencies": [
              "2.1",
              "2.2",
              "2.3"
            ],
            "details": "Set up Vitest with tsconfig paths; mock openai client module to control responses for both OpenAI and OpenRouter paths. Tests: (1) routes to correct baseURL and headers per provider; (2) enforces maxTokens cap (reject > 512); (3) retries on 429 then succeeds; (4) rejects disallowed model; (5) streaming: emit deltas across chunks and assert assembled text and usage mapping; (6) rate limiter blocks after N requests. Add ESLint + Prettier scripts and CI tasks (test, lint).\n\nTag: backend",
            "status": "done",
            "testStrategy": "Run vitest with coverage; ensure all critical branches covered. Lint passes with no warnings. Optional snapshot for normalized request payload (model, messages, max_tokens)."
          },
          {
            "id": 5,
            "title": "Context7 MCP research for external packages/libraries",
            "description": "Research and document SDK and library usage to de-risk implementation choices.",
            "dependencies": [],
            "details": "Produce docs/research-llm-provider.md summarizing: OpenAI Node SDK v4 chat.completions vs responses; streaming API patterns and how to capture usage; OpenRouter API compatibility and required headers; model naming differences and availability; lru-cache patterns suitable for rate limiting in serverless (TTL, max size, pitfalls); best practices for exponential backoff with jitter. Include links to official docs and minimal code snippets. Add decisions/risks section and confirm our API choices align with findings.\n\nTag: backend\n\n<research> OpenAI SDK v4 chat.completions vs responses; streaming iterator; OpenRouter required headers; lru-cache rate-limiter notes; backoff with jitter. Sources documented in repo research.",
            "status": "done",
            "testStrategy": "Acceptance: document exists with sources and answers to checklist; peer review confirms accuracy and that implementation aligns with recommendations."
          }
        ]
      },
      {
        "id": 3,
        "title": "Shared: Background execution APIs for battles and batch runs with persistence and streaming updates",
        "description": "Implement Next.js Route Handlers to start and monitor battles and scale tests. Provide an in-process runner by default with optional BullMQ+Redis when REDIS_URL is set. Persist conversations and messages; expose polling and basic SSE endpoints.",
        "details": "Implementation subtasks (target 5):\n1) Implement runners and routes\n- Runners (server-only): runBattle(job), runScaleTest(job). Stop when goal reached or 25 messages.\n- Goal check strategy: after each assistant reply, ask a lightweight classifier prompt (system: 'Given the goal: <goal>, did the last assistant message achieve it? Answer yes/no.') using the same provider with tiny model (e.g., gpt-4o-mini or openrouter/anthropic/haiku via OpenRouter if allowed in whitelist). Parse yes => goalReached.\n- Persistence: createConversation(agentId, model, systemPrompt, goal), appendMessage(role, content, tokensIn/out, cost), completeConversation(endedReason).\n- Routes (examples under app/api):\n  POST /api/battles/start -> { jobId }\n  GET /api/battles/:jobId/status -> { progress, conversationId, endedReason }\n  GET /api/battles/:conversationId/messages -> paginated messages\n  POST /api/scale/start -> { runId }\n  GET /api/scale/:runId/status -> { progress, completed, total }\n  GET /api/scale/:runId/report -> RunReport\n- SSE (best-effort): GET /api/stream/:conversationId -> text/event-stream using an EventEmitter; for Vercel multi-instance, keep polling as primary.\n\n2) Queue strategy + caps\n- Default: In-memory queue with p-queue or simple FIFO and concurrency=3. On Vercel, keep N small and per-request processing under timeout; for longer runs, dispatch multiple sequential calls inside the runner.\n- Optional: If process.env.REDIS_URL, initialize BullMQ Queue/Worker (Upstash Redis compatible) for durability. Name queues 'battles' and 'scale-tests'.\n- Caps: runCount <= 10, messageLimit <= 25, per-IP concurrent jobs <= 3.\n- Retry on transient LLM failures with exponential backoff.\n\n3) Cleanup\n- Ensure jobs handle abort signals; clear listeners; guard against duplicate runs; sanitize inputs.\n\n4) Quality Gate (run tests/lint)\n- Integration tests: fake provider to simulate goal reached and errors. Test stop conditions and persistence.\n- Contract tests for each route. Lint and typecheck pass.\n\n5) Context7 MCP research for external packages/libraries used\n- BullMQ vs in-process trade-offs on Vercel; Upstash Redis guidance; Next.js Route Handlers streaming (SSE) limitations and recommended polling fallback.\n\nPseudo-code (runner core):\nasync function runBattle({ agentId, model, systemPrompt, goal }) {\n  const conv = await repo.createConversation(...);\n  let i=0; let goalReached=false;\n  const sys = systemPrompt || agent.systemPrompt;\n  const msgs = [ { role:'system', content: sys }, { role:'user', content: goal || 'Begin.' } ];\n  while(i < 25 && !goalReached){\n    const res = await llm.chat(msgs, { provider, model, maxTokens: 512 });\n    await repo.appendMessage(conv.id,'assistant',res.text,res.usage?.inputTokens,res.usage?.outputTokens);\n    i++;\n    const verdict = await llm.chat([\n      { role:'system', content:`Given the goal: ${goal}. Did the last assistant message achieve it? Reply yes/no.`},\n      { role:'user', content: res.text }\n    ],{ provider, model: tinyModel, maxTokens:10 });\n    if(/\\byes\\b/i.test(verdict.text)) goalReached=true;\n  }\n  const endedReason = goalReached ? 'goal' : (i>=25 ? 'limit' : 'error');\n  await repo.completeConversation(conv.id, endedReason);\n  return { conversationId: conv.id, endedReason };\n}\n\n<info added on 2025-08-11T15:40:08.248Z>\nResearch-backed enhancements and clarifications\n\n- Architecture and runtime strategy\n  - Polling-first status updates on Vercel; reserve SSE for short-lived, interactive streams only. Default UX should poll /status and /messages incrementally; SSE is best-effort.\n  - Durable background execution: prefer BullMQ with Upstash Redis and a dedicated off-Vercel worker (e.g., Fly/Railway/Render). The Next.js app must never run a BullMQ Worker on Vercel instances; only enqueue and read job state.\n  - Fallback path: if REDIS_URL is missing or no active worker heartbeat is detected, run the in-process runner with low concurrency as a temporary fallback.\n\n- Endpoint refinements\n  - Status shape: return minimal JSON with state (queued|running|completed|failed|canceled|timeout), progress (0..1), turn/messageCount, conversationId, endedReason (if terminal), and optional counts (for scale runs: { goal, limit, error, canceled }).\n  - Incremental messages: add sinceSeq param to GET /api/battles/:conversationId/messages to stream deltas; each Message row includes a monotonically increasing seq per conversation. Response includes nextSeq for client continuation.\n  - Persist per-turn progress: after each assistant turn, persist progress fields (turn count, lastSeq, lastActivityAt, partial usage totals) so /status can respond from DB without relying on in-memory state.\n\n- Queue and worker details\n  - Singletons: initialize BullMQ Queue instances once per route module; guard with globalThis to avoid duplicate queues across hot reloads.\n  - Worker placement: only in the external worker service. Vercel routes must not instantiate Workers. Workers read from queues 'battles' and 'scale-tests'.\n  - Rate limiting: configure BullMQ limiter to avoid provider 429s (e.g., limiter { max, duration } tuned per provider/model). Keep queue concurrency aligned with provider limits.\n  - Job identity and idempotency: use conversationId as the canonical jobId; enforce idempotency by returning the existing job if the same idempotency key or conversationId is provided.\n  - Retention: set removeOnComplete and removeOnFail with age and count caps (e.g., complete: { age: 86400, count: 1000 }, fail: { age: 604800, count: 1000 }) to bound Redis memory.\n  - Health/heartbeat: maintain a Redis heartbeat key (e.g., worker:background:heartbeat:{region}) with a short TTL (30–60s). Routes check this to decide BullMQ vs in-process fallback.\n\n- Judge pipeline (per-turn)\n  - Two-stage stopping criteria: run a fast judge every turn (tiny model yes/no) and require a strong confirm before stopping on goal (e.g., larger model or stricter rubric). Deterministic validators (regex/schema/unit checks) run first; if they definitively pass/fail, skip LLM judge.\n  - Persistence: store JudgeDecision rows per turn with fields { conversationId, turn, verdict: yes|no|uncertain, confidence, criteriaApplied[], model, latencyMs, createdAt }. Expose repository helpers to append and query decisions.\n  - Stopping rule: only mark goalReached when fast judge=yes and strong confirm=yes (or deterministic pass). Conflicts default to continue-until-cap.\n\n- Cancellation and caps\n  - Cancellation flag: support a cancel key in Redis/DB (e.g., cancel:conversation:{id}). Worker/runner polls between model calls; on cancel, terminate gracefully, complete conversation with endedReason='manual'.\n  - Message cap enforcement: hard-stop at messageLimit=25 regardless of judge outcome; expose plateau detection as optional (e.g., no improvement over K turns) without changing the hard cap.\n  - Optional endpoint: POST /api/battles/:jobId/cancel sets the cancel flag and returns acknowledged=true.\n\n- References and ADR notes\n  - Document in the ADR: Vercel guidance on SSE vs polling for Route Handlers and serverless timeouts; BullMQ with Upstash Redis best practices (limiter, retention, heartbeat, external worker pattern); example configs for limiter and removeOnComplete/Fail.\n</info added on 2025-08-11T15:40:08.248Z>\n<info added on 2025-08-12T08:47:49.849Z>\nDeveloper tooling and CI baseline\n- Adopt ESLint + Prettier across this task’s codepaths. Add lint scripts and enforce no warnings (eslint --max-warnings 0) with Prettier check.\n- Use Vitest for unit/integration tests with the fake provider for runners, queues, repositories, and route handlers.\n- Add Playwright API/route smoke tests for /api/battles/*, /api/scale/*, and /api/stream/* (SSE best-effort) that validate minimal JSON contracts and status transitions.\n- Accessibility: if any status/debug UI surfaces are added for this task, include axe-core checks in Playwright to assert no serious/critical violations.\n- Performance: if any UI is shipped under this task, apply Lighthouse CI budgets per PRD “Local Tooling Stack”; otherwise skip as not applicable.\n- Makefile targets for CI gating:\n  - typecheck: pnpm typecheck\n  - lint: pnpm eslint . --max-warnings 0 && pnpm prettier --check .\n  - test: pnpm vitest run --coverage && pnpm playwright test\n- CI must gate merges by invoking make typecheck, make lint, and make test; failures block the PR.\n- Output Policy: ensure user-facing examples/docs for these APIs follow “no hidden reasoning”. If a required fact/tool is missing and a user insists, reply exactly: information unavailable.\n</info added on 2025-08-12T08:47:49.849Z>\n<info added on 2025-08-12T09:11:05.338Z>\nQuality-Gate Loop\n- a) Cleanup: remove redundant files/junk code, delete dead spikes, consolidate duplicate types; update .gitignore to exclude build artifacts and test outputs (.next, dist, coverage, playwright-report, test-results, .turbo, .vercel, .DS_Store, *.log, .env.local) and refresh README with setup/run instructions (queues, external worker, SSE vs polling, CI/Make targets).\n- b) Self-Review: inspect the diff and verify each subtask’s code exists and is sane (runners, routes, queue layer, cleanup/guards, ADR/docs). Confirm endpoint JSON contracts, caps, abort/cancel paths, and persistence fields match the spec; no secrets/PII or noisy logs; TypeScript strict, eslint/prettier clean; tests pass locally.\n- c) Git add & commit: stage changes and create descriptive commits grouped by subtask using conventional message prefixes (feat, fix, chore, docs, refactor). Summaries must explain scope and rationale. DO NOT PUSH.\n- Pre-commit enforcement: add a repo pre-commit hook that runs make quality and make test and blocks on failure. Define make quality to execute make typecheck and make lint. Ensure the hook exits non-zero on failures and prints actionable output.\n</info added on 2025-08-12T09:11:05.338Z>",
        "testStrategy": "- Integration: Start battle and assert conversation reaches 'goal' given mocked LLM; another test reaches 'limit' at 25.\n- API: Postman/contract tests for each endpoint; pagination verified.\n- Queue: Unit tests for in-memory FIFO; if REDIS_URL present in CI, spin a BullMQ worker and run one job.\n- Performance: Verify caps prevent runaway costs; simulate 10-run scale test completes under timeouts.",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement server runners: runBattle and runScaleTest with persistence and goal-check",
            "description": "Build server-only runners that execute battles and scale tests, persist conversations/messages, and determine termination via goal or message limit.",
            "dependencies": [],
            "details": "Implement runBattle(job) and runScaleTest(job) using the provider abstraction (Task 2) and repositories (Task 1). runBattle flow: createConversation(agentId, model, systemPrompt, goal) -> loop until goalReached or 25 messages -> after each assistant reply, appendMessage with tokens/cost -> run a tiny-model classifier chat to determine goalReached (yes/no) -> completeConversation(endedReason: 'goal'|'limit'|'error'). Emit progress events after each assistant turn (EventEmitter keyed by conversationId) for SSE consumers. Capture and propagate AbortSignal. runScaleTest: accept {runCount<=10, concurrency<=3, messageLimit<=25, ...}, orchestrate N battle runs, aggregate results (counts by endedReason, latency, usage totals), and persist a RunReport via repository. Ensure retries on transient LLM errors with exponential backoff and per-run token caps using provider options.\n\nTag: backend",
            "status": "pending",
            "testStrategy": "Integration tests with a fake provider: (a) goal reached on turn 1 (classifier returns 'yes'), (b) limit reached at 25 turns with classifier 'no', (c) transient error retried and then succeeds, (d) AbortSignal cancels cleanly and completes conversation with 'error' or 'timeout'. Verify create/append/complete repository calls and progress events emission."
          },
          {
            "id": 2,
            "title": "Implement Next.js Route Handlers, SSE stream, and queue layer (in-memory default, BullMQ optional)",
            "description": "Expose API endpoints to start and monitor battles/scale tests, wire runners through an in-process queue by default, and enable BullMQ+Redis when REDIS_URL is set.",
            "dependencies": [
              "3.1"
            ],
            "details": "Routes: POST /api/battles/start -> {jobId}; GET /api/battles/:jobId/status -> {progress, conversationId, endedReason}; GET /api/battles/:conversationId/messages?page&limit -> paginated; POST /api/scale/start -> {runId}; GET /api/scale/:runId/status -> {progress, completed, total}; GET /api/scale/:runId/report -> RunReport; GET /api/stream/:conversationId -> text/event-stream using Node runtime. Queue: default in-memory FIFO (e.g., p-queue) with concurrency=3; when process.env.REDIS_URL is present, initialize BullMQ Queue/Worker ('battles', 'scale-tests') compatible with Upstash Redis. Validate inputs and enforce caps: runCount<=10, messageLimit<=25, per-IP concurrent jobs<=3. Map jobId/runId to conversationId(s) and progress. For SSE: subscribe to EventEmitter per conversationId; send events on new assistant messages and on completion; document polling as the primary pattern on Vercel multi-instance. Ensure proper runtime config (runtime: 'nodejs') and guard single worker initialization in serverless environments.\n\nTag: backend",
            "status": "pending",
            "testStrategy": "Contract tests for each route: start returns IDs, status transitions as runs progress, pagination returns stable slices, report aggregates correctly. SSE: connect, receive incremental events in order, and auto-complete event on finish (fallback to polling validated by periodic status checks). Queue tests: enqueue/dequeue order honored with concurrency=3, BullMQ path executes when REDIS_URL is set (behind CI flag)."
          },
          {
            "id": 3,
            "title": "Cleanup and hardening: aborts, caps, dedupe, and input sanitization",
            "description": "Ensure robust lifecycle management, guardrails, and safe handling across runners, queues, and routes.",
            "dependencies": [
              "3.1",
              "3.2"
            ],
            "details": "Implement AbortSignal wiring end-to-end; clear EventEmitter listeners on completion/error; add idempotency keys on start endpoints to avoid duplicate runs; enforce per-IP concurrency<=3 with in-memory (and Redis-backed when available) counters; sanitize and validate agentId/model/goal inputs; redact sensitive fields from logs; implement exponential backoff and jitter for provider retries; normalize error mapping to consistent status/errors; add TTL eviction for in-memory maps (jobs, emitters) to prevent leaks; ensure BullMQ workers close gracefully on process signals. Confirm messageLimit and runCount caps enforced in both route validation and runner execution.\n\nTag: backend",
            "status": "pending",
            "testStrategy": "Simulate duplicate POST /start with same idempotency key -> single job created; exceed per-IP cap -> 429; abort mid-run -> listeners removed and resources freed; invalid inputs -> 400 with sanitized details; retry logic -> transient failures eventually succeed with capped attempts; memory leak guard -> emitters count returns to baseline after completion."
          },
          {
            "id": 4,
            "title": "Quality Gate: tests, lint, and type safety",
            "description": "Add comprehensive tests and static checks to ensure correctness and maintainability.",
            "dependencies": [
              "3.1",
              "3.2"
            ],
            "details": "Set up Vitest for unit/integration tests; add route contract tests and provider fakes; include coverage for stop conditions, persistence, pagination, queue behavior, and SSE. Add TypeScript strict mode, ESLint, and Prettier. Ensure CI runs: typecheck, lint, test. Provide minimal fixtures for Agents and Conversations for deterministic tests.\n\nTag: backend",
            "status": "pending",
            "testStrategy": "CI pipeline: pnpm typecheck, pnpm lint, pnpm test with coverage thresholds (lines>=80%). Playwright or supertest-based API tests verify endpoints; snapshot minimal JSON contracts; mocked Redis path exercised via CI flag to avoid external dependencies by default."
          },
          {
            "id": 5,
            "title": "Context7 MCP research: queue, Redis, and Next.js streaming trade-offs",
            "description": "Research external packages/libraries and platform constraints to finalize defaults and document decisions.",
            "dependencies": [],
            "details": "Produce an ADR summarizing: (1) In-memory queue vs BullMQ on Vercel (cold starts, multi-instance, durability, cost); (2) Upstash Redis specifics (connection limits, TLS, BullMQ compat, recommended settings, TTLs); (3) Next.js Route Handlers streaming constraints (Node runtime only for SSE, edge incompatibilities, buffering, headers); (4) Polling fallback cadence and backoff; (5) p-queue configuration guidance and operational caps; (6) Retry strategies for LLM providers and rate limiting considerations. Include recommended defaults and code snippets to align implementation.\n\nTag: backend",
            "status": "pending",
            "testStrategy": "Peer review of ADR; acceptance criteria: decisions are referenced in code comments, environment/runtime guidance documented, and defaults implemented match the ADR."
          }
        ]
      },
      {
        "id": 4,
        "title": "Page: Landing Page with 3D visuals and interactive node graph",
        "description": "Build the marketing/overview page with a blue futuristic theme, lightweight 3D scene (React Three Fiber + Three.js), and an interactive node graph (react-force-graph) illustrating system capabilities. Emphasize performance and lazy loading.",
        "details": "Implementation subtasks (target 5):\n1) Implement 3D + node graph UI\n- Next.js App Router page at / with TailwindCSS and shadcn/ui. Dynamic import of 3D canvas and graph to avoid SSR.\n- 3D: @react-three/fiber v9 + @react-three/drei helpers. Minimal scene: animated glTF nodes or primitives with bloom/postprocessing kept off for perf, orbit controls disabled on mobile.\n- Graph: react-force-graph (2D) with ~20 nodes representing features; hover tooltips; click to navigate to Hub/Scale/PromptBro.\n- Theme: Tailwind + CSS variables for blues; backdrop-blur panels; gradient text.\n- Accessibility: prefers-reduced-motion disables heavy animations.\n\n2) Perf and UX polish\n- Lazy load components with suspense fallbacks; use requestAnimationFrame throttling; useResizeObserver to tune dpr; Lighthouse budget.\n\n3) Cleanup\n- Remove unused shaders/assets; compress textures; ensure components are client-only where needed (\"use client\").\n\n4) Quality Gate (run tests/lint)\n- Playwright smoke test: page loads, FPS above threshold via Performance API, nav via graph clicks works.\n- ESLint/TypeScript clean.\n\n5) Context7 MCP research for external packages/libraries used\n- React Three Fiber/drei latest docs; react-force-graph rendering performance tips; Next.js dynamic import patterns for client-only libs.\n\nPseudo-code (client components):\nfunction Scene(){ /* Canvas, ambientLight, rotating meshes */ }\nfunction CapabilitiesGraph(){ /* ForceGraph2D with nodes/links, onNodeClick router.push */ }\n\n<info added on 2025-08-11T15:41:13.433Z>\nResearch-backed enhancements:\n- R3F performance\n  - Load Canvas and any Three-only code via next/dynamic (ssr:false) to keep it client-only.\n  - Default Canvas frameloop='demand'; switch to 'always' only during user interaction; on prefers-reduced-motion or when tab is hidden, use frameloop='never'.\n  - Use drei AdaptiveDpr and PerformanceMonitor to auto-tune quality; clamp DPR to min 1 and max 1.5 on typical devices (up to 1.75 on high-end). Drive clamp via ResizeObserver and simple heuristics (e.g., performance memory if available).\n  - Keep scene minimal: no shadows or postprocessing, simple materials, tiny glTF or primitives.\n  - On mobile and/or reduced motion, do not mount the Scene; render a static optimized hero image instead.\n- Graph performance\n  - Use ForceGraph2D with warmupTicks and cooldownTicks (e.g., warmup 30–60, cooldown 80–120) to settle layout quickly; after first settle, freeze layout (set cooldownTicks=0 or stopAnimation) and increase d3VelocityDecay.\n  - Implement nodeCanvasObject for custom node + label rendering; precompute/cache label text widths to avoid repeated measureText calls.\n  - Use nodePointerAreaPaint to enlarge pointer hit areas for small nodes to improve hover/click accuracy without visual changes.\n  - Render tooltip as an overlay HTML element positioned on pointermove, throttled via requestAnimationFrame; avoid triggering canvas rerenders for tooltip updates.\n  - Clamp renderer pixelRatio to ~1–1.5 for large/high-DPR cases; limit linkDirectionalParticles and other animations for stability.\n- UX and loading\n  - Lazy-load heavy assets/components behind Suspense with lightweight skeletons; set prefetch=off for 3D/graph chunks; preconnect to any external asset/CDN origins.\n  - Compress models/textures (KTX2/Draco for glTF; WebP/AVIF for images) and inline tiny assets; disable OrbitControls on mobile and node dragging on touch devices.\n- CI and quality gates\n  - Lighthouse budget remains enforced; extend Playwright FPS sanity using Performance API to assert idle FPS >= 45 and that DPR clamping is active (read devicePixelRatio).\n  - Axe-core checks include tooltip contrast and reduced-motion compliance.\n  - Add a Playwright case toggling prefers-reduced-motion to assert static fallback is rendered and 3D is not mounted.\n- References to include in /docs/landing-3d-graph.md\n  - @react-three/fiber v9 best practices (frameloop, suspense), drei AdaptiveDpr and PerformanceMonitor usage.\n  - react-force-graph optimization tips (nodeCanvasObject, nodePointerAreaPaint, warmup/cooldown, pixelRatio).\n  - Next.js dynamic import patterns for client-only libraries.\n</info added on 2025-08-11T15:41:13.433Z>\n<info added on 2025-08-12T08:48:29.357Z>\nTooling baseline and Makefile (per PRD “Local Tooling Stack”):\n- Add dev deps: eslint, eslint-config-next, eslint-config-prettier, prettier, vitest, @vitest/coverage-v8, playwright, @axe-core/playwright, lighthouse-ci.\n- Config files: .eslintrc.(js|cjs), .eslintignore, .prettierrc, .prettierignore, vitest.config.ts (jsdom for any DOM utils), playwright.config.ts (project: landing, headless by default), .lighthouserc.json pointing to lighthouse-budget.json.\n- Introduce Makefile targets used locally and in CI:\n  - make install (package install)\n  - make lint (eslint . --ext .ts,.tsx)\n  - make format (prettier --write .)\n  - make format-check (prettier --check .)\n  - make unit (vitest run --coverage)\n  - make e2e (playwright test)\n  - make a11y (playwright test -g \"a11y\" using @axe-core/playwright checks)\n  - make lhci (lhci autorun)\n  - make check (lint + format-check + unit)\n  - make ci (check + e2e + a11y + lhci; fail on any nonzero exit)\n- Vitest scope for this task: add unit tests for small utilities (e.g., DPR clamp heuristic, tooltip text width cache) to ensure deterministic behavior.\n- Playwright integrates axe checks (contrast, landmarks, reduced-motion compliance) alongside existing smoke; tag a11y tests to run via make a11y.\n- Lighthouse CI runs with existing budget (TTI, main-thread blocking) via make lhci.\n- Respect Output Policy for Prompt agents in any helper tools or examples referenced by this page: avoid live model calls in tests, mock responses, and include a brief policy note/link in /docs/landing-3d-graph.md.\n</info added on 2025-08-12T08:48:29.357Z>\n<info added on 2025-08-12T09:11:33.284Z>\nQuality-Gate Loop\n- a) Cleanup: remove redundant files/junk code; update .gitignore to exclude build artifacts and reports (e.g., .next, out, coverage, playwright-report, .lighthouseci, .DS_Store, logs); refresh README with setup, run, and Quality-Gate steps.\n- b) Self-Review: inspect the full diff; ensure each subtask’s deliverables exist and are sensible (app/(marketing)/page.tsx, components/Scene.tsx, components/CapabilitiesGraph.tsx, theme tokens, docs/landing-3d-graph.md, Makefile/configs). Sanity-check reduced-motion behavior, DPR clamping, graph navigation, and that tests/docs reference the actual implementations.\n- c) Git add & commit: stage changes and create descriptive, atomic commits per area (UI, perf, tests, docs). Do not push.\n\nPre-commit enforcement\n- Add Makefile aliases:\n  - make quality → same as make check (lint + format-check + unit).\n  - make test → same as make e2e (Playwright suite).\n- Add a pre-commit hook at .git/hooks/pre-commit to run: make quality && make test; ensure it is executable and blocks the commit on failure. Document this workflow in README and discourage --no-verify.\n</info added on 2025-08-12T09:11:33.284Z>",
        "testStrategy": "- Visual: Playwright screenshot diff for hero + graph.\n- Performance: Lighthouse CI with thresholds (TTI < 2.5s on Fast 3G emulation, main thread blocking < 250ms).\n- Accessibility: axe-core checks, reduced motion respected.\n- Navigation: clicking nodes routes to target pages.",
        "priority": "medium",
        "dependencies": [
          1,
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement landing page UI with 3D scene and interactive graph",
            "description": "Create the Next.js App Router page at '/' with TailwindCSS and shadcn/ui. Dynamically import client-only 3D Scene and ForceGraph components to avoid SSR. Build a minimal React Three Fiber v9 scene (drei helpers, ambient light, a few animated primitives or small glTF, no postprocessing) and a react-force-graph 2D graph (~20 nodes) with hover tooltips and onNodeClick routing to /hub, /scale, /promptbro. Apply blue futuristic theme (CSS variables, gradient text, backdrop-blur panels) and basic a11y (reduced motion support).",
            "dependencies": [],
            "details": "Deliverables: 1) app/(marketing)/page.tsx with hero + placeholders; 2) components/Scene.tsx (Canvas, minimal meshes, disable OrbitControls on mobile); 3) components/CapabilitiesGraph.tsx (nodes/links, custom tooltip, onNodeClick -> router.push); 4) Dynamic imports with ssr:false and suspense fallbacks; 5) Theme tokens and gradient styles. Add data-testid attributes for canvas and graph roots.\n\nTag: frontend",
            "status": "pending",
            "testStrategy": "Manual dev check: page loads without SSR errors, 3D renders, graph shows nodes with tooltips, clicking routes correctly. Verify prefers-reduced-motion disables animations. Basic TypeScript passes."
          },
          {
            "id": 2,
            "title": "Performance and UX polish (lazy load, DPR, throttling, budgets)",
            "description": "Optimize rendering and loading: lazy load heavy components with Suspense fallbacks; tune devicePixelRatio based on container size and hardware; throttle animations; set frameloop to 'demand' where possible; configure Lighthouse performance budget; ensure reduced motion disables intensive updates.",
            "dependencies": [
              "4.1"
            ],
            "details": "Implement: 1) next/dynamic with loading skeletons; 2) useResizeObserver to clamp DPR (e.g., 0.75–1.5) for Canvas; 3) conditionally pause/limit rotations and graph cooldownTicks; 4) rAF throttling for UI effects; 5) disable node drag and orbit on mobile; 6) prefetch=off for heavy routes; 7) lighthouse-budget.json with TTI < 2.5s (Fast 3G), main-thread < 250ms. Instrument simple FPS meter via requestAnimationFrame and Performance API for smoke checks.\n\nTag: frontend",
            "status": "pending",
            "testStrategy": "Run Lighthouse locally: meet budgets. Observe FPS meter >= target (e.g., >45fps on mid-tier device) when idle. Confirm lazy loading via network waterfall and bundle analyzer shows smaller initial JS."
          },
          {
            "id": 3,
            "title": "Cleanup: assets, compression, and client-only boundaries",
            "description": "Remove unused shaders/assets, compress textures/models, and ensure client-only boundaries are correct with 'use client'. Audit bundle for unused imports and heavy modules.",
            "dependencies": [
              "4.1",
              "4.2"
            ],
            "details": "Actions: 1) Purge unused drei helpers and shaders; 2) Compress textures (WebP/AVIF for images; optional KTX2/Draco for glTF if used); 3) Verify all R3F/graph components have 'use client' and are excluded from SSR; 4) Use next-bundle-analyzer to confirm reductions; 5) Code-split optional UI blocks (modals/tooltips) and remove dead code.\n\nTag: frontend",
            "status": "pending",
            "testStrategy": "Compare bundle/analyze reports before/after; ensure no runtime warnings about SSR. Spot-check visual parity post-compression."
          },
          {
            "id": 4,
            "title": "Quality Gate: tests, lint, accessibility, and performance checks",
            "description": "Add Playwright smoke tests, Lighthouse CI, axe-core accessibility checks, and ESLint/TypeScript cleanliness. Validate navigation via graph clicks and a minimum FPS threshold via Performance API.",
            "dependencies": [
              "4.1",
              "4.2",
              "4.3"
            ],
            "details": "Implement: 1) Playwright: load '/', assert [data-testid='scene-canvas'] and [data-testid='capabilities-graph'] visible; click a known node -> expect route change; 2) FPS probe script exposes window.__fpsSample for test assertions; 3) axe-core integration for key views; 4) Lighthouse CI with thresholds; 5) ESLint/TS scripts in CI.\n\nTag: frontend",
            "status": "pending",
            "testStrategy": "CI pipeline runs: Playwright smoke + screenshot for hero/graph; axe shows no serious violations; Lighthouse meets budgets; ESLint/TS pass. Fail build if any threshold unmet."
          },
          {
            "id": 5,
            "title": "Context7 MCP research: R3F/drei, react-force-graph, Next.js dynamic import",
            "description": "Research latest stable versions, performance tips, SSR caveats, and recommended props/APIs for @react-three/fiber v9, @react-three/drei, react-force-graph (2D), and Next.js dynamic imports. Produce concise notes and implementation decisions.",
            "dependencies": [],
            "details": "Deliverables: /docs/landing-3d-graph.md with: 1) Version pins and changelog notes impacting SSR or perf; 2) R3F best practices (frameloop, DPR, suspense, mobile controls); 3) react-force-graph perf tips (cooldownTicks, nodeCanvasObject, link pruning, hover/tooltip patterns); 4) Next.js dynamic import patterns for client-only libs; 5) Accessibility and reduced-motion considerations. Link primary sources.\n\nTag: frontend",
            "status": "pending",
            "testStrategy": "Acceptance: reviewed doc with links and actionable decisions referenced by code comments; verify installed versions match pins."
          }
        ]
      },
      {
        "id": 5,
        "title": "Page: Agent Wars Hub for concurrent battles",
        "description": "Create the Hub page to configure and run one or more concurrent battles between a chosen LLM model/system prompt and a prebuilt Agent from the DB. Enforce stop when goal reached or after 25 messages. Support multiple sessions with live progress.",
        "details": "Implementation subtasks (target 5):\n1) Implement Hub UI + orchestration\n- Page /hub with shadcn/ui (Select, Textarea, Button, Card, Tabs). Load agents via GET /api/agents. Form: provider, model (whitelist), agent, system prompt, goal. Start battle -> POST /api/battles/start. Maintain sessions list with jobId and conversationId.\n- Progress: Poll /api/battles/:jobId/status every 1–2s; show spinner/progress bars; fetch messages via /api/battles/:conversationId/messages for display in chat UI.\n- Allow multiple concurrent sessions (array of session cards). Stop button triggers cancel route or marks manual end.\n- Respect 25-message cap and display endedReason.\n\n2) UX and safeguards\n- Disable Start when caps exceeded; show cost estimate per session (naive tokens x model price map optional). Save last-used system prompt to localStorage.\n- Empty states and error banners; skeleton loaders. Keyboard shortcuts for send/retry.\n\n3) Cleanup\n- Debounce polling; unsubscribe on unmount. Remove console logs.\n\n4) Quality Gate (run tests/lint)\n- React Testing Library: form validation, status polling, message rendering.\n- Playwright e2e: start two sessions, see them complete with correct stop conditions (using mocked backend provider).\n\n5) Context7 MCP research for external packages/libraries used\n- shadcn/ui component APIs; Next.js caching/fetch revalidation patterns; guidance on polling vs SSE on Vercel.\n\nPseudo-code (client):\nconst start = async () => { const r = await fetch('/api/battles/start',{method:'POST',body: JSON.stringify(form)}).then(r=>r.json()); setSessions(s=>[...s,{ jobId:r.jobId }]); };\nuseEffect(()=>{ const t=setInterval(()=>sessions.forEach(syncSession)),1000); return ()=>clearInterval(t); },[sessions]);\n\n<info added on 2025-08-11T15:42:24.685Z>\nArena (blind A/B) enhancements for Hub\n- Add an Arena tab with side-by-side, blind A/B sessions. Randomize side assignment per battle; label as “A” and “B” with identical, uniform styling and no identities shown until reveal.\n- Single input broadcast: one user input box sends the same prompt to both sides simultaneously. Disable per-side editing to preserve fairness.\n- Identical generation params: enforce the same temperature, top_p, max_tokens, and stop sequences for both sides (even if one side is a prebuilt Agent). Only system prompts differ per contender.\n- Reveal Participants: add a post-outcome “Reveal Participants” control that swaps anonymized labels for real identities. Add a sticky “Select Winner” control (hidden behind a feature flag) to support future manual vote mode.\n\nOutcome logic and judging\n- Primary outcome: goal-first wins (first side to reach goal). If both reach the goal, break ties by in order: fewer turns, then fewer tokens, then lower median latency. Allow explicit tie if all tie-breakers are equal.\n- Fallback judge: optional “LLM-as-judge” button that evaluates both final transcripts via a small model and returns structured JSON: { winner: 'A'|'B'|'tie', reasoning: string, scores: { helpfulness: number, correctness: number, conciseness: number } }. Do not reveal identities in the judge prompt; include the goal and both transcripts with side tags only. Parse JSON strictly with graceful failure and surface judge reasoning in UI.\n\nFairness and budget controls\n- Identity masking: hide model/agent names, provider logos, and any distinguishing UI until reveal; ensure uniform bubble style, timestamps, and avatars across sides.\n- Length normalization: in outcome display, show normalized efficiency metrics (tokens/turn, latency/turn) and use them only as tie-breakers (do not bias the live display).\n- Per-session budget cap: allow setting a spend cap (USD) per arena battle; display live spend per side using the existing price map and token usage when available. Auto-stop a side that hits its cap and mark endedReason=“limit:budget”.\n\nPolling and incremental fetching\n- Poll cadence: 1–2s while running; if no changes detected for 3 consecutive polls, back off to 3–5s; stop polling when terminal. Resume fast polling on any change.\n- Incremental messages: prefer GET /api/battles/:conversationId/messages?sinceSeq=<n> to fetch only new messages. Track lastSeq per side; status endpoint should include lastSeq and current spend/tokens if available. Fallback to full fetch only if sinceSeq unsupported.\n- Avoid long-lived SSE; keep short-lived polling with staggered intervals across sessions to reduce contention.\n\nRatings preparation (optional, future-ready)\n- On outcome (manual or judge), emit an arenaOutcome event payload { battleId, contenderA: {...}, contenderB: {...}, winner: 'A'|'B'|'tie', metrics: { turns, tokens, latency }, judge: { scores, reasoning }? } to a lightweight client-side queue and POST /api/arena/outcomes (mock or no-op for now).\n- Leave hooks to aggregate offline via Elo/BTL/TrueSkill. Plan a future Rating table (contenderId, mu, sigma, wins, losses, ties, lastUpdated) and store outcomes in RunReport for reproducibility.\n\nResearch references\n- Expand docs/hub-research.md with a section summarizing LMSYS Chatbot Arena UI/UX and evaluation patterns: blind A/B, single-input broadcast, reveal-after-vote, tie allowance, and rating aggregation methods (Elo/BTL/TrueSkill). Include links and prompt/rubric examples for LLM-as-judge.\n</info added on 2025-08-11T15:42:24.685Z>\n<info added on 2025-08-12T08:49:49.380Z>\nTooling baseline and CI integration (per PRD “Local Tooling Stack”)\n- ESLint + Prettier\n  - Add .eslintrc.cjs extending next/core-web-vitals, plugin:@typescript-eslint/recommended, and prettier; enable react-hooks rules; set env: browser, node, es2023.\n  - Add .prettierrc with project defaults (semi: true, singleQuote: false, printWidth: 100, trailingComma: all); include .prettierignore for build, .next, coverage, and generated files.\n  - Add npm scripts: lint, lint:fix, format, format:check.\n- Vitest + React Testing Library + MSW for Hub UI\n  - Add vitest.config.ts with jsdom environment, alias to src, coverage enabled, and setupFiles: tests/setupTests.ts.\n  - In tests/setupTests.ts: install @testing-library/jest-dom, polyfill fetch (whatwg-fetch), and configure MSW (server.listen()/resetHandlers()/close()).\n  - Co-locate Hub unit/integration tests under app/hub/__tests__ with .test.tsx files; use MSW handlers for /api/agents, /api/battles/*.\n- Playwright e2e (multi-session)\n  - Add playwright.config.ts with baseURL (dev server), headless in CI, trace on-first-retry, video on-failure, and workers=2–4.\n  - Create e2e specs covering starting two concurrent sessions, progress to goal/25 cap, Stop flow; mock network via route.fulfill or start app with test-only routes.\n- Accessibility checks with axe\n  - Unit-level: add jest-axe to Vitest; include an a11y.test.tsx for /hub and Arena tab ensuring no serious/critical violations on initial render and during active sessions.\n  - E2E-level: integrate @axe-core/playwright to scan /hub (standard and Arena) after hydration; fail on violations >= serious.\n- Lighthouse budgets (optional)\n  - Add lighthouserc.json with budgets for /hub (transfer size, script size) and basic performance thresholds; run in CI only when LHCI=1.\n- Makefile targets for local and CI\n  - make install: install deps\n  - make lint / make lint-fix\n  - make format / make format-check\n  - make typecheck: run tsc --noEmit\n  - make test:unit: run Vitest once with coverage\n  - make test:watch: run Vitest in watch mode\n  - make test:e2e: start dev server and run Playwright headless\n  - make a11y: run axe checks (Vitest jest-axe + Playwright axe scans)\n  - make lh-ci: run Lighthouse CI if LHCI=1\n  - make ci: lint, typecheck, test:unit, build, test:e2e, a11y, and optionally lh-ci\n- Output Policy compliance for embedded assistants\n  - Label AI-generated content (e.g., judge reasoning) clearly in UI; avoid revealing identities in judge prompts; suppress provider/model identities in Arena until reveal.\n  - Do not log raw prompts/responses in console; scrub PII in diagnostics; ensure any examples shown in tests/fixtures comply with policy.\n</info added on 2025-08-12T08:49:49.380Z>\n<info added on 2025-08-12T09:12:25.917Z>\nQuality-Gate Loop (pre-commit enforced)\n- Cleanup: remove redundant files, dead code, unused mocks/fixtures; update .gitignore (e.g., .env*, .vercel, .DS_Store, .next, dist, coverage, artifacts, playwright-report) and README with local setup, scripts, and how to run Hub/Arena and tests.\n- Self-Review: inspect the diff and verify each subtask’s implementation exists and is coherent (UI wiring, polling/backoff, caps, Arena masking/tie-breakers, budgets, tests, a11y). Remove console/debug logs and TODOs not addressed; ensure naming/types/error handling are consistent; check a11y labels and keyboard shortcuts.\n- Git add & commit: stage logical chunks and write descriptive commit messages summarizing work and referencing subtasks; DO NOT PUSH.\n\nPre-commit hook\n- Add a repo pre-commit hook that blocks commits on failure by running: make quality && make test.\n- If absent, add Makefile targets:\n  - quality: runs lint, format-check, and typecheck.\n  - test: runs unit/integration tests (Vitest/MSW) only; exclude e2e (Playwright) from the hook.\n- The hook should print a concise failure summary and exit non-zero on any error.\n</info added on 2025-08-12T09:12:25.917Z>",
        "testStrategy": "- Unit: Validate form, model whitelist, and disabling logic.\n- Integration: Mock API to simulate job progressing from 0->100 and verify UI updates.\n- E2E: Run with fake provider to assert multiple sessions can run concurrently and stop at goal or 25.",
        "priority": "medium",
        "dependencies": [
          1,
          2,
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Hub UI and session orchestration",
            "description": "Build /hub page with shadcn/ui form to configure and start one or more battles, manage sessions list, and display live conversations.",
            "dependencies": [],
            "details": "Create app/hub/page.tsx with shadcn/ui components (Select, Textarea, Input, Button, Card, Tabs). On mount, fetch agents via GET /api/agents and populate agent Select with loading skeletons and error banner on failure. Form fields: provider (openai|openrouter), model (filtered by whitelist for provider), agent (from DB list), system prompt (Textarea), goal (Textarea/Input). Start battle handler: POST /api/battles/start with {provider, model, agentId, systemPrompt, goal}; on success, append a new session card with {jobId, conversationId, provider, model, agentName, startedAt}. For each session card, show header with agent/model, goal, and status chip. Implement polling loop (setInterval 1–2s) per active session to GET /api/battles/:jobId/status; when status returns conversationId (if not given initially) or progress, update session state. Fetch messages for each session via /api/battles/:conversationId/messages to render a simple chat thread (assistant/user roles). Show endedReason when status indicates terminal state (goal|limit|manual|error|timeout) and disable controls for that session. Support multiple concurrent sessions by maintaining an array of sessions in state and rendering independent cards.\n\nTag: frontend",
            "status": "pending",
            "testStrategy": "Unit: mock GET /api/agents and POST /api/battles/start to verify form submission creates a session with jobId and displays initial card. Integration: mock status endpoint to progress from running to goal/limit and assert UI reflects progress and terminal state; verify multiple sessions render independently."
          },
          {
            "id": 2,
            "title": "UX and safeguards (validation, costs, persistence, controls)",
            "description": "Harden the Hub experience with validation, disabled states, cost estimate, local persistence, and shortcuts.",
            "dependencies": [
              "5.1"
            ],
            "details": "Implement client-side validation: require provider, model (must be in whitelist for selected provider), agent, goal, and optionally system prompt; show inline errors. Disable Start when invalid or if simple caps exceeded (e.g., estimated tokens > model cap). Add naive cost estimate per session near Start button using a static model price map and a simple token estimate (length-based heuristic). Persist last-used system prompt and selected provider/model in localStorage; hydrate on load. Add empty states for no agents and no sessions, error banners for failed actions, and skeleton loaders for initial agents fetch. Add keyboard shortcuts: Cmd/Ctrl+Enter to Start battle when valid; S to Stop selected session. Add Stop button per session: call /api/battles/:jobId/cancel if available; if not, mark manual end locally and stop polling for that session. Surface endedReason prominently and show message count badge, ensuring 25-message cap is communicated.\n\nTag: frontend",
            "status": "pending",
            "testStrategy": "Unit: verify Start button disables with invalid form or cap exceeded; validate price and token estimate display; ensure localStorage hydration and persistence. Integration: simulate cancel endpoint success/failure and assert UI updates and errors. Accessibility: check focus management and keyboard shortcuts trigger expected handlers."
          },
          {
            "id": 3,
            "title": "Cleanup and performance polish",
            "description": "Stabilize polling and resource usage; remove debug output and ensure clean unmount.",
            "dependencies": [
              "5.1",
              "5.2"
            ],
            "details": "Refactor polling to avoid thundering herd: debounce or stagger per-session intervals; pause polling when a session reaches a terminal state; backoff on transient 429/5xx with capped retry. Ensure intervals/timeouts and any AbortControllers are cleared on unmount and when a session stops. Guard against duplicate polling by keying intervals by jobId. Batch state updates to minimize re-renders (e.g., use functional setState and React batching). Remove console logs and add minimal structured debug behind a DEBUG flag if needed.\n\nTag: frontend",
            "status": "pending",
            "testStrategy": "Unit: verify cleanup removes intervals on unmount and when session ends; assert no duplicate polls are scheduled after re-renders. Integration: simulate 429/500 responses and ensure backoff occurs without UI freeze."
          },
          {
            "id": 4,
            "title": "Quality Gate: tests, lint, and e2e",
            "description": "Add automated tests and static checks to ensure reliability of the Hub page.",
            "dependencies": [
              "5.1",
              "5.2",
              "5.3"
            ],
            "details": "React Testing Library: form validation (required fields, model whitelist), Start disabled logic, error banners, and cost estimate rendering. Polling and status: mock status progression from 0→100 and verify spinner/progress and endedReason. Message rendering: mock messages API and assert chat thread updates. Playwright e2e: start two sessions with a mocked backend provider, observe concurrent progress, and verify they stop at goal or 25 messages; test Stop button. Add lint (eslint) and type checks (tsc) to CI. Include mock service worker (MSW) or test-only route mocks to control API responses deterministically.\n\nTag: frontend",
            "status": "pending",
            "testStrategy": "Unit/Integration: RTL with jest/vitest and MSW for API mocking. E2E: Playwright with deterministic fixtures for agents, start, status, messages, and cancel routes. CI: run eslint and tsc; collect coverage for critical paths."
          },
          {
            "id": 5,
            "title": "Context7 MCP research for external packages and patterns",
            "description": "Research and document component APIs and platform patterns to guide implementation decisions.",
            "dependencies": [],
            "details": "Produce docs/hub-research.md summarizing: shadcn/ui component APIs used (Select, Textarea, Button, Card, Tabs) with gotchas for controlled inputs and accessibility; Next.js fetch caching/revalidation patterns (no-store vs revalidate tags) appropriate for client-side polling and when to use server actions; guidance on polling vs SSE on Vercel (limits, connection longevity, edge vs node runtimes), with recommended default of polling every 1–2s and optional SSE behind a feature flag. Include links to official docs and example snippets. Record chosen model whitelist strategy and price map sources.\n\nTag: frontend",
            "status": "pending",
            "testStrategy": "Review: PR checklist requires link to the research doc; spot-check that cited APIs match versions used; validate that implementation follows documented guidance (polling cadence, caching flags)."
          }
        ]
      },
      {
        "id": 6,
        "title": "Page: Scale Testing for batch runs and summarized report",
        "description": "Provide a Scale Testing page to configure model/system prompt/run count, select an Agent, execute N background conversations, persist results, and display a summarized RunReport including failures, prompt issues, and a revised prompt.",
        "details": "Implementation subtasks (target 5):\n1) Implement Scale UI + run orchestration\n- Page /scale with form: provider, model, agent, system prompt, goal (optional), runCount (cap <= 10). POST /api/scale/start -> runId.\n- Show run progress via GET /api/scale/:runId/status; display per-run conversation links; when complete, fetch /api/scale/:runId/report and render summary + revised prompt with copy-to-clipboard.\n\n2) Summarization workflow\n- After all runs, compile failures and issues. Use LLM to summarize and propose revised prompt. Store in RunReport.\n- Summarizer prompt outline: \"You are PromptBro Analyst. Given N conversations with outcomes and failure cases, produce: 1) failure modes, 2) hallucinations, 3) goal failures, 4) character inconsistency, 5) revised prompt with rationale.\" Limit tokens to stay under caps.\n\n3) Cleanup\n- Prevent duplicate submissions; guard against leaving page mid-run; clear intervals on unmount.\n\n4) Quality Gate (run tests/lint)\n- Integration: Mock scale run that completes and returns a deterministic summary. Validate revised prompt rendering.\n- E2E: User configures 5 runs, sees progress, and final report.\n\n5) Context7 MCP research for external packages/libraries used\n- Prompt engineering patterns for automated summarization; token budgeting strategies; UI patterns for progress displays.\n\nPseudo-code (server summarizer):\nconst summary = await llm.chat([{ role:'system', content: SUMMARY_SYSTEM_PROMPT }, { role:'user', content: JSON.stringify(runsLite) }], { provider, model: summarizerModel, maxTokens: 512 });\nawait repo.saveRunReport({ runId, revisedPrompt: extractPrompt(summary.text), summary: summary.text, stats });\n\n<info added on 2025-08-11T15:43:50.571Z>\nResearch-backed enhancements\n\nEvaluation\n- Integrate Promptfoo as the primary evaluator for completed conversations. For each transcript, run assertions for: goal adherence, toxicity, refusal, and formatting. Normalize Promptfoo results to EvaluationResult { assertion, pass, score?, details } and derive FailureTag[] per conversation.\n- FailureTag mapping: goal adherence -> GOAL_MISS; toxicity -> TOXICITY; refusal -> REFUSAL; formatting -> FORMAT_ERROR. Allow additional tags via config.\n- Optional Python evaluator behind feature flag (FEATURE_EVAL_PY=1) using DeepEval/TruLens. Expose a simple HTTP contract POST /eval with { conversationId, transcript, goal } -> { results: EvaluationResult[] }. Merge back into the same FailureTag schema. Fallback to Promptfoo when flag is off or worker is unavailable.\n\nReporting extensions\n- Extend RunReport.stats to include:\n  - winRate (successes/total), averageTurns, p50/p95 turns\n  - tokenStats: prompt/completion/total (avg, p95), totals across batch\n  - latencyStats: avg, p95, max (per-turn and per-run if available)\n  - cost: estimatedUsd, actualUsd (if token usage returned), deltaUsd\n  - topFailureModes: [{ tag, count, sampleConversationIds }]\n  - judgeStats: per-assertion pass/fail counts and passRate\n- Revised prompt generation: require concise rationale ≤100 words; include rationale in report.revisedPromptRationale. Keep JSON-structured summarizer output to reliably parse prompt and rationale.\n- Include per-conversation evaluation summary in report payload: [{ conversationId, failureTags, assertionResults }].\n\nCost and concurrency controls\n- Preflight budget: before starting runs, estimate total cost using dynamic pricing for the selected provider/model. Cache pricing with TTL and fall back to configured defaults if lookup fails. Show estimatedUsd and require confirmation if exceeding user-supplied maxBudgetUsd.\n- Caps: enforce per-run and total budget caps. Abort or pause additional launches when projected or actual cost would exceed caps; include a clear error state in UI.\n- Concurrency: throttle to 3 (default) up to 5 max; stagger launches with 200–400ms jitter to reduce 429. On 429/5xx, apply exponential backoff with jitter and bounded retries. Surface throttling state in status API.\n- API changes:\n  - POST /api/scale/start accepts { maxBudgetUsd?, maxConcurrency? (<=5), evaluator?: 'promptfoo'|'python' } and returns { runId, estimatedUsd }.\n  - GET /api/scale/:runId/status includes { spendingUsd?, throttling?: { active: boolean, concurrency, queueDepth } }.\n  - GET /api/scale/:runId/report includes extended stats, judgeStats, topFailureModes, revisedPromptRationale, and per-conversation evaluation summaries.\n\nUI enhancements\n- Progress polling and per-run conversation links as implemented; add:\n  - Budget and cost card: estimated vs actual with a progress bar and cap indicators.\n  - Summary badges for common failure modes (GOAL_MISS, TOXICITY, REFUSAL, FORMAT_ERROR) with counts; clicking filters the per-run list.\n  - Stats cards for win-rate, avg turns, p95 latency, and total tokens/cost.\n  - Copy-to-clipboard for revised prompt (existing), plus a separate copy for concise rationale.\n  - Non-blocking toast/inline alerts when throttling/backoffs occur or budget caps are hit.\n\nImplementation notes\n- Promptfoo config lives alongside code (e.g., /evaluators/promptfoo.config.ts) and is generated per-run with assertion thresholds and optional judge model. Run programmatically, not via CLI, to gather structured results.\n- Provider pricing: fetch dynamically (OpenAI/OpenRouter) via provider adapter; cache in-memory with periodic refresh and allow overrides via config.\n- Concurrency/backoff: use a queue (e.g., p-queue) with concurrency control and jittered scheduling; centralize 429 detection and retry policy.\n\nAdditional tests\n- Unit: mapping from Promptfoo results to FailureTag; budget estimator with dynamic pricing and caps; revisedPrompt rationale length enforcement; pricing cache TTL behavior.\n- Integration: mocked Promptfoo run producing mixed pass/fail; verify FailureTag aggregation, judgeStats, and topFailureModes in RunReport; budget preflight abort; concurrency throttling with simulated 429/backoffs.\n- E2E: user sets maxBudgetUsd, starts 5 runs, sees throttling indicators if simulated 429s occur, final report shows badges/stats and copy works.\n\nReferences deliverable\n- Produce a short research doc comparing Promptfoo, DeepEval, and TruLens (criteria: ease of integration, assertion breadth, cost, determinism, maintenance). Include pricing best practices for OpenAI/OpenRouter (dynamic lookup, conservative estimation, caching, and fallbacks) and recommended defaults (concurrency=3, p95 metrics tracked, rationale ≤100 words).\n</info added on 2025-08-11T15:43:50.571Z>\n<info added on 2025-08-12T08:51:17.979Z>\nTooling baseline and CI\n- Adopt ESLint + Prettier per PRD Local Tooling Stack. Add package.json scripts: lint, lint:fix, format, format:check. Use a unified .eslintrc with TypeScript plugin and eslint-config-prettier; add .prettierrc. Gate on lint and format:check in CI.\n- Testing: use Vitest + MSW for server utilities and API adapters (replace Nock in new tests). Centralize MSW handlers for provider pricing lookups, evaluator endpoints, and scale APIs.\n- E2E: use Playwright for the Scale flow. Integrate axe a11y checks (@axe-core/playwright) in the Scale page flow; fail on violations above minor severity. Optional Lighthouse budgets via lhci with a budgets.json file; run only when LIGHTHOUSE=1.\n- Makefile-based CI:\n  - make check: typecheck, lint, format:check, unit + integration tests (Vitest with MSW), lightweight a11y check on /scale using Playwright + axe in headless mode.\n  - make ci: make check, then full Playwright E2E for Scale, optional Lighthouse budgets if LIGHTHOUSE=1. Wire CI to call make ci.\n\nSummarizer output policy\n- Update the summarizer system prompt and schema to enforce:\n  - No hidden reasoning or chain-of-thought in user-visible output; only return the required JSON fields.\n  - Rationale must be concise (≤100 words).\n  - If any required fact or tool result is unavailable, do not fabricate; set the affected field(s) to the exact string \"information unavailable\".\n- Add unit tests asserting the prompt builder includes these rules, the parser rejects extra freeform reasoning, enforces rationale length, and correctly handles \"information unavailable\" values.\n</info added on 2025-08-12T08:51:17.979Z>\n<info added on 2025-08-12T09:13:05.976Z>\nQuality-Gate Loop\n- Cleanup: remove dead/unused files and commented code; update .gitignore (e.g., coverage/, .vite/, .turbo/, playwright-report/, test-results/, .lhci/, .DS_Store) and README (Scale page usage, evaluator flags, budget caps, pre-commit hook).\n- Self-Review: inspect the diff and verify each subtask is implemented and sane (UI /scale form + progress and links; start/status/report APIs; summarizer with structured JSON and rationale ≤100 words; cleanup/resilience guards; evaluation/reporting extensions; budget/concurrency controls; tooling/CI additions).\n- Git add & commit: group logical changes per subtask; write descriptive, scoped commit messages (e.g., scale, api, eval, ui, ci); DO NOT PUSH.\n- Pre-commit enforcement: add a repo pre-commit hook that runs make quality and make test; block commit on failure.\n- Makefile targets: quality runs lint, format:check, and typecheck; test runs Vitest unit+integration with MSW (no E2E) to keep the hook fast.\n</info added on 2025-08-12T09:13:05.976Z>",
        "testStrategy": "- Unit: Summarizer prompt builder outputs deterministic structure.\n- Integration: End-to-end mocked scale run storing conversations and RunReport.\n- UI snapshot tests for summary and revised prompt blocks.\n- Limits: Verify runCount cap and error display when exceeded.",
        "priority": "medium",
        "dependencies": [
          1,
          2,
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Scale UI and run orchestration APIs",
            "description": "Build the /scale page with configuration form and wire up start/status/report endpoints to orchestrate N background conversations and display progress and results.",
            "dependencies": [],
            "details": "UI: /scale page (shadcn/ui) with fields provider, model, agentId, systemPrompt, goal (optional), runCount (1–10). Validate with zod, disable submit while running, and persist runId in state. APIs: POST /api/scale/start -> { runId }; GET /api/scale/:runId/status -> { status, completed, total, conversationIds }; GET /api/scale/:runId/report -> { summary, revisedPrompt, stats }. Orchestration: on start, launch up to N conversations concurrently using existing agent/conversation services; persist Conversation/Message rows; maintain an in-memory registry for run status and per-run conversationIds; mark complete when all settle. UI: poll status every 1–2s, show progress bar and per-run conversation links; on completion, fetch report and render summary and revised prompt with copy-to-clipboard.\n\nTag: frontend",
            "status": "pending",
            "testStrategy": "Unit: zod schema validates fields and runCount cap; status/report response schemas. Integration: mock start/status/report to simulate progress and completion; verify UI disables submit and shows per-run links. Snapshot: summary and revised prompt blocks."
          },
          {
            "id": 2,
            "title": "Summarization workflow and RunReport persistence",
            "description": "Compile results from completed runs, invoke LLM summarizer, extract a revised prompt, and store a RunReport for retrieval.",
            "dependencies": [
              "6.1"
            ],
            "details": "Build runsLite payload: for each conversation include agent/model, goal, goalReached, endedReason, error messages, and trimmed messages/meta (token-capped). Summarizer: call provider abstraction with a system prompt (PromptBro Analyst) producing sections: failure modes, hallucinations, goal failures, character inconsistency, and a revised prompt with rationale; cap maxTokens (e.g., 512), low temperature, retries/backoff. Enforce structured JSON output for reliable parsing; extract revisedPrompt and rationale. Persist RunReport { runId, summary, revisedPrompt, stats (success/fail counts, token/cost if available) }. Expose via GET /api/scale/:runId/report.\n\nTag: backend",
            "status": "pending",
            "testStrategy": "Unit: deterministic prompt builder and JSON parser; revisedPrompt extractor with edge cases. Integration: mock LLM to return fixed structured output; verify RunReport saved and served by /report."
          },
          {
            "id": 3,
            "title": "Cleanup and resilience for Scale runs",
            "description": "Prevent duplicate submissions, guard navigation mid-run, clear polling, and harden API idempotency and caps.",
            "dependencies": [
              "6.1"
            ],
            "details": "Client: disable Start while running; include idempotencyKey on POST; add beforeunload guard during active runs; clear polling intervals on unmount; abort in-flight fetches on route change. Server: enforce runCount cap (<=10) with 400 on violation; honor idempotencyKey to return existing runId; timeouts and safe error shaping; tolerate server restarts by recomputing status from DB when possible and falling back to registry.\n\nTag: frontend",
            "status": "pending",
            "testStrategy": "Unit: double-submit guard reducer; idempotencyKey resolver; cap enforcement. Integration: repeated POST with same idempotencyKey returns same runId; verify intervals cleared with fake timers; navigation guard appears only while running."
          },
          {
            "id": 4,
            "title": "Quality Gate: tests, lint, typecheck, and E2E",
            "description": "Add comprehensive tests and CI checks to ensure reliable behavior from configuration to final report.",
            "dependencies": [
              "6.1",
              "6.2",
              "6.3"
            ],
            "details": "Unit: form validation (caps/errors), summarizer prompt builder/parser, revised prompt extractor, endpoint schema validators. Integration: mocked scale run that progresses deterministically and returns a fixed summary; validate UI renders progress, per-run links, and final report. UI snapshot: summary and revised prompt blocks. E2E (Playwright): configure 5 runs, observe progress to completion, assert revised prompt rendered and copy-to-clipboard works. CI: run lint (ESLint), format (Prettier), and tsc typecheck.\n\nTag: frontend",
            "status": "pending",
            "testStrategy": "Run unit/integration in Vitest with MSW/Nock for HTTP; Playwright for browser flow; CI pipeline gates on lint, typecheck, and tests."
          },
          {
            "id": 5,
            "title": "Context7 MCP research: libraries and patterns",
            "description": "Research external packages and patterns for background orchestration, token budgeting, summarization prompting, and progress UI to inform implementation.",
            "dependencies": [],
            "details": "Background jobs: compare p-queue/in-memory vs BullMQ/Bree for persistence; start with in-memory, plan BullMQ if needed. Token estimation: js-tiktoken or gpt-tokenizer to budget/truncate runsLite; design to sample/trim messages. Summarization prompts: structured JSON output, avoid chain-of-thought, include concise rationale; fallback strategies when over token budget. UI progress patterns: shadcn/ui Progress/Table, polling vs SSE (begin with polling). Validation and DX: zod for schemas, superjson if needed for rich data, navigator.clipboard with copy-to-clipboard fallback. Document decisions and snippets for adoption.\n\nTag: frontend",
            "status": "pending",
            "testStrategy": "Produce a brief research doc with chosen defaults, tradeoffs, and minimal PoCs (token counter, progress component) reviewed by the team."
          }
        ]
      },
      {
        "id": 7,
        "title": "Page: PromptBro guided prompt creator",
        "description": "Build a guided prompt creation experience where users input free text and interact with a custom LLM agent that asks clarifying questions. Save resulting prompts as PromptTemplates for reuse. Leave hooks for future checks/fixes.",
        "details": "Implementation subtasks (target 5):\n1) Implement guided creator UI + backend\n- Page /promptbro with multi-step wizard (shadcn/ui Steps/Tabs): Context, Constraints, Style, Evaluation Criteria. Live preview of assembled prompt.\n- Custom LLM assistant: /api/promptbro/assist that takes current draft and returns next best question or improvement suggestion (uses provider abstraction, small model, capped tokens).\n- Save as PromptTemplate via POST /api/prompts (name, template, variables). List saved templates.\n\n2) Draft assembly + versioning\n- Maintain prompt draft in state with templating variables like {{goal}} {{constraints}}. Provide export and copy.\n- Version saved drafts (append timestamp) and allow restore.\n\n3) Cleanup\n- Persist last draft in localStorage. Remove experimental flags. Ensure server routes validate inputs with zod.\n\n4) Quality Gate (run tests/lint)\n- Unit: Assistant returns a question given a partial draft (mocked LLM). Template save/restore works.\n- E2E: Create and save a prompt, reload page, and see it listed.\n\n5) Context7 MCP research for external packages/libraries used\n- Prompt engineering guidance for question-driven refinement; shadcn/ui patterns for wizards; content security for user text.\n\nPseudo-code (assist route):\nPOST /api/promptbro/assist ->\n  const { draft } = await req.json();\n  const sys = 'You help craft prompts by asking one clarifying question at a time...';\n  const res = await llm.chat([{ role:'system', content: sys }, { role:'user', content: draft }], { provider, model: smallModel, maxTokens: 128 });\n  return { question: res.text };\n\n<info added on 2025-08-11T15:44:37.598Z>\nResearch-backed enhancements:\n\n- Evaluation Criteria generation and persistence\n  - In the Evaluation Criteria step, add a Derive Evaluation Criteria action that POSTs to /api/promptbro/eval-criteria with the current assembled prompt and step context, returning a deterministic JSON checklist.\n  - JSON shape (validated with zod) includes: checklist[] items with id, name, description, weight 0..1 (default 1), required (bool); optional scoring with method (weighted|binary|rubric) and passingThreshold 0..1.\n  - Allow inline editing of the returned checklist before save. Persist this as evaluationCriteria on POST /api/prompts alongside { name, template, variables }. Include evaluationCriteria in GET list responses.\n\n- Assist streaming over SSE (Edge)\n  - Add /api/promptbro/assist/stream running on Edge runtime and emitting Server-Sent Events with a heartbeat every 15–30s. Events: message (partial tokens or final question), heartbeat, done, error.\n  - Default small model gpt-4o-mini (or equivalent via OpenRouter). Enforce strict caps: maxTokens <= 256 and per-request budget <= $0.002. Perform preflight cost estimation using the provider price map; reject with 402/400 if budget would be exceeded. Keep temperature low.\n  - Retain existing non-streaming /api/promptbro/assist for fallback; UI auto-selects SSE when supported and falls back on error.\n\n- Quick checks before save (Promptfoo-style mini suite)\n  - Add /api/promptbro/checks that runs a fast heuristic suite to flag: ambiguity (missing audience, success criteria, timeframe, data sources), prompt injection patterns (e.g., ignore previous, system:, jailbreak markers), output format risks (JSON shape mismatches if evaluationCriteria exists), and constraint coverage (unreferenced required tokens).\n  - Returns { issues: [{ id, severity: low|medium|high, message, hint }], score: 0..100 }.\n  - UI adds Run Quick Checks; on high severity, show a warning and allow override to save.\n\n- Persistence and quotas\n  - Local draft autosave remains with 500ms debounce; surface last autosave time in UI.\n  - Add a configurable per-user daily cap for assist calls (fallback to per-IP when unauthenticated). Env: PROMPTBRO_ASSIST_DAILY_CAP (default 50). Enforce across both /assist and /assist/stream; return 429 when exceeded. Use Redis if available; otherwise in-memory map with TTL.\n\n- Validation and schemas\n  - Extend POST /api/prompts zod schema to accept optional evaluationCriteria JSON and validate its structure.\n  - Add zod schemas for /api/promptbro/eval-criteria input/output and for /api/promptbro/checks input.\n\n- Test strategy additions\n  - Unit: evaluationCriteria schema validation; budget guard rejects when estimated cost > $0.002; quick checks correctly flag known bad cases and pass clean prompts.\n  - Integration: SSE endpoint streams message and heartbeat events; non-streaming fallback works; daily cap triggers 429 after N calls; evaluationCriteria persists and round-trips via GET.\n  - E2E: Derive Evaluation Criteria, edit, save template, reload and see criteria present; run Quick Checks and observe warnings; stream an assist response and see live updates.\n\n- Documentation additions (docs/context7-promptbro.md)\n  - SSE vs polling trade-offs for low-latency, low-QPS interactions; heartbeat recommendations and fallback strategy.\n  - Pricing guardrails: preflight estimation, maxTokens caps, per-request budget, and daily quota patterns.\n  - Judge pipeline: using evaluationCriteria JSON to drive validators and rubric-based judges in future workflows.\n</info added on 2025-08-11T15:44:37.598Z>\n<info added on 2025-08-12T08:52:34.556Z>\nTooling baseline and Makefile\n- Adopt ESLint + Prettier with TypeScript/React/Next presets; add npm scripts: lint, lint:fix, format, format:check.\n- Testing: Vitest as the test runner with jsdom for component tests; React Testing Library for PromptBro UI; MSW for API mocking (/api/promptbro/* and /api/prompts); coverage thresholds enabled.\n- E2E: Playwright for UX flow of /promptbro; record video/screenshot on failure.\n- Accessibility: axe checks via jest-axe (Vitest compatible) for component tests and axe-playwright for E2E a11y smoke.\n- Makefile targets:\n  - install (pnpm|npm i)\n  - lint (eslint .)\n  - format (prettier --write .) and format-check (prettier --check .)\n  - test (vitest run), test-ui (vitest --ui)\n  - e2e (playwright test)\n  - a11y (vitest a11y suite + playwright a11y)\n  - check (lint + format-check + test)\n  - ci (non-interactive aggregate used by CI)\n\nPromptBro Output Policy enforcement\n- Policy: The assistant must not reveal hidden reasoning or chain-of-thought. If a required fact or tool is missing and the user insists on proceeding, respond exactly: information unavailable\n- Apply in both /api/promptbro/assist and /api/promptbro/assist/stream:\n  - System prompt additions: produce only a single clarifying question or a single concrete improvement suggestion; never include reasoning steps, internal rules, or system message content; if asked to reveal reasoning or to proceed without required facts/tools, respond exactly: information unavailable\n  - Preflight guard: if draft is missing required variables or external tools are unavailable for the user’s request, short-circuit and return information unavailable without calling the provider.\n  - Post-process guard: strip or reject outputs containing chain-of-thought markers (e.g., “let’s think step by step”, “reasoning:”, “analysis:”, “thought:”, “chain-of-thought”) and replace with a compliant single question; if the content attempts to fabricate missing facts, return information unavailable\n  - SSE-specific: if a violation is detected mid-stream, stop upstream generation and emit a single message event with information unavailable followed by done. Heartbeats remain unaffected.\n  - Logging: never log model outputs beyond minimal metadata; do not persist any redacted content.\n\nTests for tooling and policy\n- Vitest unit:\n  - Output policy: returns information unavailable when required variables are missing or tools are unavailable and user insists.\n  - CoT suppression: when the provider mock returns step-by-step reasoning, the API responds with a single question and no CoT markers.\n- Integration:\n  - SSE stream adheres to policy (no CoT markers; short-circuits to information unavailable when preflight fails); non-streaming fallback matches behavior.\n  - MSW-backed tests for /assist, /assist/stream, /prompts CRUD.\n- E2E (Playwright):\n  - Full /promptbro flow passes; a11y smoke via axe has no critical violations on each step.\n  - Policy scenarios: attempting to coerce hidden reasoning shows information unavailable in the assistant panel.\n- Documentation: add sections to docs/context7-promptbro.md for Local Tooling Stack (how to run Makefile targets) and Output Policy (rules, guards, and examples).\n</info added on 2025-08-12T08:52:34.556Z>\n<info added on 2025-08-12T09:14:03.732Z>\nQuality-Gate Loop\na) Cleanup: remove redundant files and junk code; update .gitignore to exclude build artifacts and local files (e.g., .next, dist, coverage, .env*, .DS_Store, playwright-report, test-results); refresh README to reflect current setup and how to run lint/format/tests and Makefile targets.\nb) Self-Review: inspect git diff; verify each subtask’s code, tests, and docs exist and are sane (UI steps, APIs, schemas/zod, SSE, checks, persistence, tooling, and docs). Remove TODO/debug code, dead files, and ensure naming/exports are consistent.\nc) Git add & commit: stage logical chunks and create descriptive commit messages summarizing work; DO NOT PUSH.\n\nPre-commit enforcement\n- Add a Makefile target quality that runs lint and format-check (and typecheck if available) or alias it to check.\n- Add a repo pre-commit hook that runs: make quality && make test, aborting the commit on failure.\n</info added on 2025-08-12T09:14:03.732Z>",
        "testStrategy": "- Unit: Validate draft builder merges steps correctly and variables render.\n- Integration: Assist route returns a string and UI displays it; saving creates a PromptTemplate row.\n- UX: Playwright flow through steps, save, and restore template.",
        "priority": "medium",
        "dependencies": [
          1,
          2
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Build /promptbro wizard UI and Assist API",
            "description": "Implement the guided prompt creator page with a multi-step wizard, live preview, and an assist endpoint that asks clarifying questions. Enable saving prompts as PromptTemplates and listing existing templates.",
            "dependencies": [],
            "details": "- Page: Next.js App Router page at /promptbro using shadcn/ui Steps/Tabs for steps: Context, Constraints, Style, Evaluation Criteria. Maintain form state per step and a combined draft object.\n- Live preview: Debounced assembled prompt preview panel; highlight templating tokens like {{goal}} {{constraints}} {{style}} {{evaluation}}.\n- Assist API: POST /api/promptbro/assist with { draft: string|object }. Use provider abstraction (Task 2) if available; otherwise stub a provider adapter. Small model, capped tokens (<=128), temperature low. System prompt to ask one clarifying question at a time or provide a single improvement suggestion. Return { question: string }.\n- Save template: POST /api/prompts with { name, template, variables[] }. Add a list view (GET /api/prompts) to show saved templates with basic pagination. Provide Save button from the wizard with name input.\n- UI affordances: Ask Assistant button, display latest question and a free-text answer box that, on submit, merges into the draft.\n\nTag: frontend",
            "status": "pending",
            "testStrategy": "- Integration: Mock provider to ensure /api/promptbro/assist returns a string question; UI displays it.\n- Contract: Validate POST /api/prompts creates a PromptTemplate row and GET lists it.\n- UX smoke: Step navigation updates preview without errors."
          },
          {
            "id": 2,
            "title": "Draft assembly, templating, versioning, and export",
            "description": "Implement deterministic draft assembly with templating variables, lightweight versioning with timestamped snapshots, restore, and export/copy utilities.",
            "dependencies": [
              "7.1"
            ],
            "details": "- Assembly: Compose final template from step fields into a single prompt string with tokens {{goal}} {{context}} {{constraints}} {{style}} {{evaluation}}. Provide a helper buildPrompt(draft) that is pure and testable.\n- Variables: Extract tokens used in the template and surface them as variables[] for saving. Warn if required tokens are empty.\n- Versioning: Add Save Version action that stores up to N (e.g., 10) timestamped snapshots; enable one-click restore into the current draft.\n- Export: Copy-to-clipboard for the assembled prompt; export JSON with { name, template, variables }.\n- Minimal UI: Versions sidebar/list integrated into the wizard; restore confirmation.\n\nTag: frontend",
            "status": "pending",
            "testStrategy": "- Unit: buildPrompt merges steps correctly; variable extraction returns a stable set. Version save/restore round-trips.\n- Snapshot: Assembled prompt remains stable given the same draft."
          },
          {
            "id": 3,
            "title": "Cleanup: persistence, validation, and safeguards",
            "description": "Persist last draft locally, remove experimental flags, and validate server inputs with zod. Add simple rate/token caps and user-facing error handling.",
            "dependencies": [
              "7.1",
              "7.2"
            ],
            "details": "- Persistence: localStorage key promptbro:lastDraft; load on mount with schema guard; debounce saves (e.g., 500ms). Provide Reset Draft action.\n- Validation: zod schemas for /api/promptbro/assist and template save payloads (name length, variables format, template size limits). Return 400 on invalid input.\n- Safeguards: Enforce maxTokens <= 128 and basic in-memory rate limit (e.g., 10/min per IP) on assist route; trim overly long drafts.\n- Cleanup: Remove temporary feature flags, ensure no prompt contents are logged beyond minimal metadata.\n- UX: Error toasts and retry CTA; loading states for assist and save.\n\nTag: frontend",
            "status": "pending",
            "testStrategy": "- Unit: zod validators reject malformed payloads; persistence loads valid drafts and ignores corrupt ones.\n- Integration: Assist route enforces caps and returns 429 when rate-limited."
          },
          {
            "id": 4,
            "title": "Quality Gate: tests, lint, and E2E flow",
            "description": "Add unit, integration, and E2E tests; wire up lint/typecheck in CI. Validate the full user flow from creation to save and reload.",
            "dependencies": [
              "7.1",
              "7.2",
              "7.3"
            ],
            "details": "- Unit: Mocked LLM provider ensures assist returns a single question; buildPrompt and versioning helpers are covered.\n- Integration: API tests for /api/promptbro/assist and /api/prompts with input validation and happy paths.\n- E2E (Playwright): Navigate /promptbro, fill steps, preview updates, call Assist, save as template, reload page, and see the saved template listed.\n- CI: Add npm scripts for lint, typecheck, unit, and e2e (mocked); GitHub Actions workflow runs lint + unit on PRs.\n\nTag: frontend",
            "status": "pending",
            "testStrategy": "- Commands: npm run lint, npm run typecheck, npm run test, npm run e2e (with mocks).\n- Artifacts: Playwright screenshots for preview; assert template appears post-reload."
          },
          {
            "id": 5,
            "title": "Context7 MCP research and implementation notes",
            "description": "Research external packages/patterns for question-driven prompt refinement, wizard UX with shadcn/ui, and content security. Produce a concise repo doc with recommendations and hooks for future checks/fixes.",
            "dependencies": [],
            "details": "- Prompt engineering: Summarize best practices for iterative question-asking and constraint elicitation; recommend small models and caps for assist (e.g., compact models via OpenAI/OpenRouter, 128–256 tokens, low temperature).\n- UI patterns: Document shadcn/ui Tabs/Steps usage, accessibility and keyboard navigation, and minimal state management tips.\n- Security: Guidance on handling untrusted user text (escape on render, no HTML injection, size limits), PII minimization, and logging hygiene.\n- Packages: Confirm versions for shadcn/ui, zod, and any form helpers; list minimal dependencies to install.\n- Hooks: Define analyzeDraft(draft) and fixDraft(draft) placeholders for future quality/safety checks; stubbed with no-ops today.\n- Deliverable: docs/context7-promptbro.md with links and actionable checklists.\n\nTag: frontend",
            "status": "pending",
            "testStrategy": "- Doc lint: Ensure the markdown exists and passes link checks. Reviewer checklist to validate recommendations are reflected in implementation."
          },
          {
            "id": 6,
            "title": "Reuse existing /api/llm/chat as Assist backend",
            "description": "In the /promptbro wizard, call POST /api/llm/chat for clarifying Q&A instead of adding a new endpoint. Keep prompts client-side and avoid secrets.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 7,
            "title": "Add repo readers for prompt templates",
            "description": "Implement listPromptTemplates() and getPromptTemplateByName() in src/repo/promptTemplates.ts and surface them to the wizard for listing/restoring drafts.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7
          }
        ]
      },
      {
        "id": 8,
        "title": "Follow-ups: finalize agent-wars root merge (docs/hooks/ci)",
        "description": "Add missing docs/examples/hooks to ensure the merged configuration fully works. Connected to: agent-wars merge/cleanup ticket.",
        "status": "pending",
        "priority": "low",
        "subtasks": [
          {
            "id": 1,
            "title": "Create .env.example and update README for local Postgres (docs)",
            "description": "Add src/agents_wars/web/.env.example with DATABASE_URL/DIRECT_URL placeholders and update README with non-Docker setup steps. Connected to: agent-wars merge ticket. Type: docs",
            "status": "done"
          },
          {
            "id": 2,
            "title": "Add pre-commit hook to run make quality (chore)",
            "description": "Configure .git/hooks/pre-commit to run make -C src/agents_wars quality. Connected to: agent-wars merge ticket. Type: chore",
            "status": "done"
          },
          {
            "id": 3,
            "title": "Add CI step to run agent_wars check (ci)",
            "description": "Ensure CI runs make -C src/agents_wars check. Connected to: agent-wars merge ticket. Type: ci",
            "status": "done"
          },
          {
            "id": 4,
            "title": "Integration test: cascade delete on Message->Conversation (test)",
            "description": "Add a Vitest integration test to assert deleting a Conversation cascades to Messages. Connected to: agent-wars merge ticket. Type: test",
            "status": "done"
          }
        ]
      },
      {
        "id": 9,
        "title": "API: LLM chat route handler (provider smoke test)",
        "description": "Add Next.js Route Handler to call unified LLM provider (POST /api/llm/chat). Connected to agent-wars Task 2. Type: backend",
        "priority": "medium",
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement POST /api/llm/chat",
            "description": "Zod-validate input; call provider.chat; non-stream JSON, stream SSE",
            "status": "done"
          },
          {
            "id": 2,
            "title": "Add minimal integration test or manual script",
            "description": "Ensure both providers work with mock keys; document usage",
            "status": "done"
          }
        ]
      },
      {
        "id": 10,
        "title": "UI: Debug page to exercise LLM provider API",
        "description": "Add a minimal Next.js page to POST to /api/llm/chat (non-stream and SSE) to validate provider/guards end-to-end. Connected to agent-wars Task 2 (provider). Type: frontend",
        "priority": "low",
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create /debug/llm page",
            "description": "Form for provider/model/messages; invoke API; render response or streamed text",
            "status": "done"
          },
          {
            "id": 2,
            "title": "Error handling and validation",
            "description": "Show validation errors and rate limit messages; no secrets in UI",
            "status": "done"
          }
        ]
      },
      {
        "id": 11,
        "title": "Tests: Integration/E2E for /api/llm/chat (SSE + JSON)",
        "description": "Add integration tests for the chat Route Handler: validate zod errors (400), JSON path, and SSE streaming with assembled deltas. Connected to Task 2 (provider/API). Type: backend",
        "priority": "medium",
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Vitest integration test for JSON path",
            "description": "Mock OpenAI client; assert {text,usage} shape and 200",
            "status": "done"
          },
          {
            "id": 2,
            "title": "SSE streaming test",
            "description": "Mock streaming iterator; assert deltas and done event",
            "status": "done"
          },
          {
            "id": 3,
            "title": "400 invalid body test",
            "description": "Send invalid body; assert 400 + details",
            "status": "done"
          }
        ]
      },
      {
        "id": 12,
        "title": "UI: Provider key status banner + reusable selector",
        "description": "Add a small banner to warn when provider keys are missing and a reusable provider/model selector component used by debug UI (and future pages). Connected to Task 2. Type: frontend",
        "priority": "low",
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "KeyStatusBanner",
            "description": "Check env via a small /api/config or attempt; render non-sensitive status only",
            "status": "pending"
          },
          {
            "id": 2,
            "title": "ProviderModelSelector",
            "description": "Controlled component for provider+model with basic validation",
            "status": "pending"
          },
          {
            "id": 3,
            "title": "GET /api/config/provider-status",
            "description": "Return non-sensitive flags: { openaiConfigured, openrouterConfigured, allowedModels } for client banner/selector. Use getConfig() and never expose keys.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 12
          },
          {
            "id": 4,
            "title": "KeyStatusBanner component",
            "description": "Client component that fetches /api/config/provider-status and renders warnings when keys are missing; no secrets displayed.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 12
          },
          {
            "id": 5,
            "title": "Extract ProviderModelSelector",
            "description": "Refactor provider/model controls from /debug/llm page into a reusable controlled component with basic validation and allowedModels support.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 12
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-08-11T14:34:00.374Z",
      "updated": "2025-08-13T13:40:00.433Z",
      "description": "Tasks for agent-wars context"
    }
  }
}
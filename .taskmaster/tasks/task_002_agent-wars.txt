# Task ID: 2
# Title: Shared: LLM provider abstraction (OpenAI + OpenRouter) with unified chat and cost/safety guards
# Status: in-progress
# Dependencies: None
# Priority: high
# Description: Create a provider layer that unifies chat completion across OpenAI and OpenRouter, exposing a single interface with model selection, token caps, and basic rate limiting. Include retries and optional streaming support.
# Details:
Implementation subtasks (target 5):
1) Implement provider interface and adapters
- Install openai@^4.55.0 (latest major v4). Use the same client for OpenRouter by overriding baseURL and apiKey.
- Interface:
  type ChatMessage = { role: 'system'|'user'|'assistant'|'tool', content: string };
  type ChatOptions = { provider: 'openai'|'openrouter', model: string, maxTokens?: number, temperature?: number, stop?: string[], stream?: boolean };
  async function chat(messages: ChatMessage[], opts: ChatOptions): Promise<{ text: string, usage?: { inputTokens: number, outputTokens: number }, raw: any }>
- OpenAI example:
  const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
  const resp = await client.chat.completions.create({ model: opts.model, messages, max_tokens: opts.maxTokens ?? 512, temperature: opts.temperature ?? 0.2 });
- OpenRouter example:
  const client = new OpenAI({ apiKey: process.env.OPENROUTER_API_KEY, baseURL: 'https://openrouter.ai/api/v1' });
  const resp = await client.chat.completions.create({ model: opts.model, messages, max_tokens: opts.maxTokens ?? 512, temperature: opts.temperature ?? 0.2, extra_headers: { 'HTTP-Referer': process.env.OPENROUTER_SITE || '', 'X-Title': 'Agent Wars' } });
- Map usage tokens from resp.choices[0] and resp.usage.

2) Cost/safety guards and retries
- Enforce caps: maxTokens per call <= 512 (configurable), max messages per conversation <= 25, allowed models whitelist.
- Basic rate limiting with in-memory LRU (per IP/per API key) using lru-cache; fallback no-op in serverless multi-instance, but keep caps. Exponential backoff retries with abort after 2 retries on 429/5xx.
- Redact secrets from prompts via simple regex before logging.

3) Cleanup
- Centralize env validation with zod: OPENAI_API_KEY? OPENROUTER_API_KEY? Ensure helpful error messages.

4) Quality Gate (run tests/lint)
- Vitest unit tests mocking OpenAI client. Cases: OpenAI vs OpenRouter path, token cap enforced, retry on 429, model not allowed.

5) Context7 MCP research for external packages/libraries used
- OpenAI Node SDK v4 docs (Responses vs Chat Completions—use chat.completions for broader model support). OpenRouter API compatibility notes. lru-cache best practices in serverless environments.

<info added on 2025-08-11T15:38:40.961Z>
Research-backed enhancements to provider layer and guards:

- OpenRouter integration
  - Include required headers HTTP-Referer and X-Title on every OpenRouter request (chat and models).
  - Add GET /models fetch with 6h TTL cache (lru-cache). Cache fields: model id, context_length (tokens), and pricing.prompt/completion (USD per 1M tokens). Expose getModelInfo(model) -> { contextTokens, promptUSDPerMTok, completionUSDPerMTok }.
  - Normalize all pricing computations to per-1M tokens: cost = (prompt_tokens/1e6)*promptUSDPerMTok + (completion_tokens/1e6)*completionUSDPerMTok. Surface context length via getModelInfo for guard checks and observability.
  - For OpenAI models, seed a static pricing map for common models with env override (OPENAI_PRICING_JSON) and include context lengths; fall back to conservative defaults if unknown.

- Guardrails and limits
  - Preflight token estimate using tokenizer encodings:
    - o200k_base for gpt-4o family (gpt-4o, gpt-4.1, o3, mini variants).
    - cl100k_base for gpt-4 (legacy), gpt-3.5, and similar.
    - Fallback heuristic for unknown models. Reject when estimated inputTokens + requested maxTokens > model contextTokens.
  - Budget caps: enforce MAX_USD_PER_REQUEST and MAX_USD_PER_SESSION (config). Estimate pre-call cost using pricing and token estimate; hard-stop if exceeding budget. Session budget sourced from persisted Conversation totals when available; allow per-call override (opts.budgetUSD) for ephemeral sessions.
  - Model whitelist remains required; also validate against the cached /models catalog when provider=openrouter to catch unavailable models.
  - Concurrency limiter keyed by provider:model with configurable limits:
    - Defaults via CONCURRENCY_DEFAULT (e.g., 4).
    - Per-key overrides via CONCURRENCY_OVERRIDES CSV (e.g., openai:gpt-4o=2,openrouter:anthropic/claude-3-haiku=10).
    - Implement a keyed semaphore/queue to bound concurrent calls fairly.
  - Retries/backoff: up to 2 retries on 429/5xx with exponential backoff and full jitter. If Retry-After header is present, honor it (seconds or HTTP-date), capped by a MAX_BACKOFF_MS ceiling. Include jitter on all backoff delays.

- Streaming API
  - Add chatStream(messages, opts) for streaming use-cases:
    - opts.streamMode: 'iterator' returns an AsyncIterable of delta chunks.
    - opts.streamMode: 'web' returns a Web ReadableStream suitable for Next.js/SSE routes.
    - Preserve existing chat() for non-streaming; both paths share guards and retries (retry only on initial call).
  - Enforce per-request timeout with AbortController (REQUEST_TIMEOUT_MS, default 60000) applied to streaming and non-streaming. Ensure streams terminate within ~60s; abort and surface a timeout error if exceeded.

- Usage and cost accounting
  - Reconcile token usage via response.usage (prompt_tokens, completion_tokens). For streaming, capture final usage when provided by SDK; if missing, fall back to preflight estimate for budgeting only and mark usageEstimated=true.
  - Compute usdIn/usdOut/totalUSD using normalized per-1M pricing from getModelInfo or OpenAI pricing map.
  - Persist on Message: inputTokens, outputTokens, usdIn, usdOut, totalUSD, provider, model, usageEstimated flag. Aggregate on Conversation and RunReport (incremental totals and counts).

- Tests (extend Vitest suite)
  - OpenRouter: assert HTTP-Referer and X-Title headers on chat and models requests; verify /models cache with ~6h TTL (no refetch within TTL, refetch after expiry).
  - Guardrails: token estimation blocks requests exceeding context; budget caps reject when estimated cost exceeds per-request/session budget; whitelist rejections; concurrency limiter never exceeds configured parallelism.
  - Retries/backoff: validate exponential backoff with jitter and honoring Retry-After (use fake timers).
  - Pricing: verify normalization per 1M tokens and correct USD computation from a sample usage.
  - Streaming: chatStream returns AsyncIterable and Web ReadableStream variants; iterator yields ordered deltas; timeout aborts long-running streams.

- Config additions
  - REQUEST_TIMEOUT_MS (default 60000), PRICING_CACHE_TTL_HOURS (default 6), MAX_USD_PER_REQUEST, MAX_USD_PER_SESSION, CONCURRENCY_DEFAULT, CONCURRENCY_OVERRIDES, OPENAI_PRICING_JSON (optional), RETRY_MAX_BACKOFF_MS (cap for Retry-After and exponential backoff).

- References for research doc (subtask 5)
  - OpenRouter best practices: required headers, /models endpoint schema, pricing fields, context_length.
  - OpenAI pricing and token estimation: official pricing tables and tokenizer encodings (cl100k_base, o200k_base) and guidance on usage.
</info added on 2025-08-11T15:38:40.961Z>
<info added on 2025-08-12T08:47:02.728Z>
Tooling baseline and testing alignment
- Adopt repo-wide tooling: ESLint (configs: @typescript-eslint/recommended+stylistic, eslint-plugin-react/recommended, react-hooks/recommended, jsx-a11y/recommended, import/recommended+typescript, eslint-config-prettier), Prettier with prettier-plugin-tailwindcss, Vitest, React Testing Library, MSW, Playwright with axe-core integration, and Lighthouse CI.
- Provider-layer tests must use Vitest with full SDK mocks (mock the openai module and baseURL/header branching; no real network). MSW may be used for ancillary HTTP (e.g., OpenRouter /models) but still mocked/deterministic in unit tests.
- Add an e2e smoke via Playwright that runs behind mocked routes only (use Playwright route interception or MSW in browser) to verify provider API endpoints and streaming behavior without hitting external LLMs.
- Integrate Lighthouse CI in CI pipeline (not specific to provider UI but enforced repo-wide thresholds); add axe checks to Playwright flows.

Prompt Agent Output Policy compliance
- No hidden reasoning: never log, persist, or emit chain-of-thought/internal rationale. Redaction already applies to prompts; extend it to strip any reasoning-style traces from logs/analytics if present.
- Provide a reusable policy system message constant for callers (POLICY_SYSTEM_PROMPT) that instructs models to not reveal chain-of-thought and to summarize reasoning only if needed.
- When a required fact or tool is unavailable and the caller explicitly requests hidden reasoning or insists on it, the provider-facing helper should return exactly: information unavailable. Expose a lightweight enforcePolicy(content: string, context: { missingToolOrFact: boolean; userInsists: boolean }) utility used by higher layers before sending or after receiving model text to comply with the PRD “Local Tooling Stack”.
</info added on 2025-08-12T08:47:02.728Z>
<info added on 2025-08-12T09:10:07.364Z>
Quality-Gate Loop
- a) Cleanup: remove redundant/scaffold files and dead code; prune unused mocks/fixtures; update .gitignore to exclude build/test artifacts (e.g., .env.*, .next, dist, coverage, playwright-report, .lighthouseci) and add any new ones introduced; refresh README to document new env/config keys (REQUEST_TIMEOUT_MS, PRICING_CACHE_TTL_HOURS, MAX_USD_PER_REQUEST, MAX_USD_PER_SESSION, CONCURRENCY_DEFAULT, CONCURRENCY_OVERRIDES, OPENAI_PRICING_JSON, RETRY_MAX_BACKOFF_MS), setup, and test/run instructions.
- b) Self-Review: inspect the diff and verify each subtask’s code exists and is sane:
  - Provider adapters: OpenAI/OpenRouter routing, required OpenRouter headers, chat() and chatStream() behavior, usage mapping.
  - Guards: caps, whitelist, model catalog validation, token preflight/context checks, budget caps, concurrency limiter, retries/backoff honoring Retry-After, redaction, POLICY_SYSTEM_PROMPT and enforcePolicy utility, streaming timeout.
  - Config: zod validation with helpful errors and defaults; single source of truth used by provider/guards.
  - Tests: Vitest with openai SDK mocks; MSW for /models; fake timers for backoff; pricing normalization; streaming iterator/web modes; rate limiting and concurrency bounds.
  - Docs/tooling: research doc present; repo tooling aligned with baseline.
- c) Git add & commit: stage logical chunks and create descriptive commits summarizing work per area (e.g., provider, guards, config, tests, docs, tooling). DO NOT PUSH.

Pre-commit enforcement
- Add a repo pre-commit hook that runs: make quality and make test; block commits on failure. Document hook installation in README (e.g., .git/hooks/pre-commit script or Husky).
</info added on 2025-08-12T09:10:07.364Z>

# Test Strategy:
- Unit tests validate both providers and edge cases (caps, errors, retries).
- Contract tests: Snapshot minimal request/response mapping.
- Smoke test against real APIs gated by CI secret flags (avoid cost by running only on demand).
- Security: Ensure no PII/logging of prompt content beyond hashes in tests.

# Subtasks:
## 1. Implement provider interface and adapters (OpenAI + OpenRouter) [review]
### Dependencies: None
### Description: Create a unified chat() using OpenAI Node SDK v4 for both OpenAI and OpenRouter, supporting non-streaming and optional streaming, with normalized response mapping.
### Details:
Install openai@^4.55.0. Define types: type ChatMessage = { role: 'system'|'user'|'assistant'|'tool', content: string }; type ChatOptions = { provider: 'openai'|'openrouter', model: string, maxTokens?: number, temperature?: number, stop?: string[], stream?: boolean }. Export async function chat(messages: ChatMessage[], opts: ChatOptions): Promise<{ text: string, usage?: { inputTokens: number, outputTokens: number }, raw: any }>. Use the same OpenAI client for both providers; for OpenAI: new OpenAI({ apiKey: process.env.OPENAI_API_KEY }); for OpenRouter: new OpenAI({ apiKey: process.env.OPENROUTER_API_KEY, baseURL: 'https://openrouter.ai/api/v1' }). For OpenRouter, pass extra_headers: { 'HTTP-Referer': process.env.OPENROUTER_SITE || '', 'X-Title': 'Agent Wars' }. Call client.chat.completions.create({ model: opts.model, messages, max_tokens: opts.maxTokens ?? 512, temperature: opts.temperature ?? 0.2, stop: opts.stop, stream: opts.stream }). Non-streaming: return choices[0].message.content || ''. Streaming: consume streamed chunks, append delta content to build final text, and capture final usage when available; resolve with assembled text and usage. Normalize usage to { inputTokens: resp.usage?.prompt_tokens ?? 0, outputTokens: resp.usage?.completion_tokens ?? 0 }. Return { text, usage, raw: resp }.

Tag: backend

<plan added on 2025-08-13T09:16:53.883Z>
Plan for 2.1:
- Add openai@^4.55.0 to web.
- Implement src/lib/llm/provider.ts with unified chat() supporting openai and openrouter via baseURL switch.
- Export types ChatMessage, ChatOptions, and chat().
- Typecheck and simple smoke test later.


## 2. Add cost/safety guards, rate limiting, and retries [review]
### Dependencies: 2.1
### Description: Enforce token and message caps, model whitelist, basic in-memory rate limiting, exponential backoff retries on 429/5xx, and prompt redaction before logging.
### Details:
Implement guard layer wrapping chat(): caps: maxTokens per call <= 512 (configurable via env, default 512); max messages per conversation <= 25; reject if exceeded. Enforce allowed models whitelist from env (e.g., ALLOWED_MODELS=comma-separated); default to a safe small set if unset. Add simple in-memory rate limiter using lru-cache keyed by IP or API key with sliding window (e.g., 60 requests/60s configurable) and TTL; if serverless multi-instance detected, allow opt-out via RATE_LIMIT_ENABLED=false but keep caps. Implement exponential backoff with jitter for 429/5xx: up to 2 retries (total 3 attempts) with delays ~250ms, 750ms. Redact secrets in any logs via regex (e.g., api keys, bearer tokens, emails) before printing. Expose minimal metadata (attempts, provider) in raw for observability. Ensure streaming path also passes through guards and retries (retry only on initial call errors, not mid-stream).

Tag: backend

## 3. Cleanup: Centralize environment and config validation [done]
### Dependencies: 2.1
### Description: Create a config module using zod to validate environment variables and runtime options with clear error messages.
### Details:
Add config.ts with zod schemas: At least one of OPENAI_API_KEY or OPENROUTER_API_KEY must be present; OPENROUTER_SITE optional; ALLOWED_MODELS optional CSV -> string[]; MAX_TOKENS_PER_CALL default 512; MAX_MESSAGES_PER_CONVO default 25; RATE_LIMIT_ENABLED default true; RATE_LIMIT_RPM default 60. Export getConfig() returning typed config and helpers. Validate at module load and throw descriptive errors with remediation hints. Replace scattered env reads in provider/guards with config getters.

Tag: backend

## 4. Quality Gate: tests and lint [done]
### Dependencies: 2.1, 2.2, 2.3
### Description: Add Vitest unit tests mocking OpenAI client, plus lint/format checks. Cover provider routing, caps, retries, whitelist, and streaming assembly.
### Details:
Set up Vitest with tsconfig paths; mock openai client module to control responses for both OpenAI and OpenRouter paths. Tests: (1) routes to correct baseURL and headers per provider; (2) enforces maxTokens cap (reject > 512); (3) retries on 429 then succeeds; (4) rejects disallowed model; (5) streaming: emit deltas across chunks and assert assembled text and usage mapping; (6) rate limiter blocks after N requests. Add ESLint + Prettier scripts and CI tasks (test, lint).

Tag: backend

## 5. Context7 MCP research for external packages/libraries [done]
### Dependencies: None
### Description: Research and document SDK and library usage to de-risk implementation choices.
### Details:
Produce docs/research-llm-provider.md summarizing: OpenAI Node SDK v4 chat.completions vs responses; streaming API patterns and how to capture usage; OpenRouter API compatibility and required headers; model naming differences and availability; lru-cache patterns suitable for rate limiting in serverless (TTL, max size, pitfalls); best practices for exponential backoff with jitter. Include links to official docs and minimal code snippets. Add decisions/risks section and confirm our API choices align with findings.

Tag: backend

<research> OpenAI SDK v4 chat.completions vs responses; streaming iterator; OpenRouter required headers; lru-cache rate-limiter notes; backoff with jitter. Sources documented in repo research.

